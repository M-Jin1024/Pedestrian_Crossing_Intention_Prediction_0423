(tf26) minshi@Legion:~/Pedestrian_Crossing_Intention_Prediction$  cd /home/minshi/Pedestrian_Crossing_Intention_Prediction ; /usr/bin/env /home/minshi/miniconda3/en
vs/tf26/bin/python /home/minshi/.vscode/extensions/ms-python.debugpy-2025.14.1-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 54641 -- /home/minshi/Pedestrian_Crossing_Intention_Prediction/train_and_test_all_epoch_pipeline.py -c config_files/my/my_jaad.yaml                                                        ================================================================================
ğŸ¯ è®­ç»ƒå’Œæµ‹è¯•ç®¡é“å¯åŠ¨
================================================================================
é…ç½®æ–‡ä»¶: config_files/my/my_jaad.yaml
å¼€å§‹æ—¶é—´: 2025-11-12 05:36:13

ğŸš€ å¼€å§‹è®­ç»ƒ...
2025-11-12 05:36:18.834617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:36:18.841267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:36:18.841873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             config_files/my/my_jaad.yaml
model_opts {'model': 'Transformer_depth', 'obs_input_type': ['box', 'depth', 'vehicle_speed', 'ped_speed'], 'enlarge_ratio': 1.5, 'obs_length': 16, 'time_to_event':
 [30, 60], 'overlap': 0.8, 'balance_data': False, 'apply_class_weights': True, 'dataset': 'jaad', 'normalize_boxes': True, 'generator': True, 'fusion_point': 'early', 'fusion_method': 'sum'}                                                                                                                                          data_opts {'fstride': 1, 'sample_type': 'beh', 'subset': 'default', 'data_split_type': 'default', 'seq_type': 'crossing', 'min_track_size': 76}
net_opts {'num_hidden_units': 256, 'global_pooling': 'avg', 'regularizer_val': 0.0001, 'cell_type': 'gru', 'backbone': 'vgg16', 'dropout': 0.1}
train_opts {'batch_size': 2, 'epochs': 30, 'lr': 5e-05, 'learning_scheduler': {}}
---------------------------------------------------------
Generating action sequence data
fstride: 1
sample_type: beh
subset: default
height_rng: [0, inf]
squarify_ratio: 0
data_split_type: default
seq_type: crossing
min_track_size: 76
random_params: {'ratios': None, 'val_data': True, 'regen_data': False}
kfold_params: {'num_folds': 5, 'fold': 1}
---------------------------------------------------------
Generating database for jaad
jaad database loaded from /home/minshi/Pedestrian_Crossing_Intention_Prediction/JAAD/data_cache/jaad_database.pkl
---------------------------------------------------------
Generating crossing data
Split: train
Number of pedestrians: 324 
Total number of samples: 194 
---------------------------------------------------------
Generating action sequence data
fstride: 1
sample_type: beh
subset: default
height_rng: [0, inf]
squarify_ratio: 0
data_split_type: default
seq_type: crossing
min_track_size: 76
random_params: {'ratios': None, 'val_data': True, 'regen_data': False}
kfold_params: {'num_folds': 5, 'fold': 1}
---------------------------------------------------------
Generating database for jaad
jaad database loaded from /home/minshi/Pedestrian_Crossing_Intention_Prediction/JAAD/data_cache/jaad_database.pkl
---------------------------------------------------------
Generating crossing data
Split: val
Number of pedestrians: 48 
Total number of samples: 22 
---------------------------------------------------------
Generating action sequence data
fstride: 1
sample_type: beh
subset: default
height_rng: [0, inf]
squarify_ratio: 0
data_split_type: default
seq_type: crossing
min_track_size: 76
random_params: {'ratios': None, 'val_data': True, 'regen_data': False}
kfold_params: {'num_folds': 5, 'fold': 1}
---------------------------------------------------------
Generating database for jaad
jaad database loaded from /home/minshi/Pedestrian_Crossing_Intention_Prediction/JAAD/data_cache/jaad_database.pkl
---------------------------------------------------------
Generating crossing data
Split: test
Number of pedestrians: 276 
Total number of samples: 171 
[DataGenerator] auto class_weight -> {0: 0.8247422680412371, 1: 0.17525773195876287}
[DataGenerator] auto class_weight -> {0: 0.7272727272727273, 1: 0.2727272727272727}
2025-11-12 05:36:22.341812: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN
) to use the following CPU instructions in performance-critical operations:  AVX2 FMA                                                                               To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-12 05:36:22.343710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:36:22.343935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:36:22.344070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:36:22.874503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:36:22.874708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:36:22.874845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:36:22.874968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3834 MB memory
:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9                                                        
============================================================
ğŸ“Š MODEL PARAMETER STATISTICS
============================================================
Total parameters:        2,968,717
Trainable parameters:    2,968,711
Non-trainable parameters: 6
============================================================

/home/minshi/miniconda3/envs/tf26/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_ra
te` instead.                                                                                                                                                          warnings.warn(

ğŸš€ Training started!
ğŸ“ Models will be saved to: data/models/jaad/Transformer_depth/12Nov2025-05h36m22s
ğŸ“‹ å·²å¤åˆ¶ action_predict.py åˆ°æ¨¡å‹ç›®å½•
2025-11-12 05:36:25.380352: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/30
2025-11-12 05:36:34.234253: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged
 once.                                                                                                                                                              2025-11-12 05:36:34.855515: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100
1067/1067 [==============================] - 30s 20ms/step - loss: 9.9111 - cls_loss: 0.2118 - reg_loss: 0.0127 - intention_accuracy: 0.5375 - val_loss: 5.9273 - va
l_cls_loss: 0.2815 - val_reg_loss: 0.0043 - val_intention_accuracy: 0.5579                                                                                          /home/minshi/miniconda3/envs/tf26/lib/python3.8/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must over
ride get_config. When loading, the custom mask layer must be passed to the custom_objects argument.                                                                   warnings.warn('Custom mask layers require a config and must override '
[Sigma] Epoch 1: sigma_cls=0.8424 sigma_reg=1.5921
Epoch 2/30
1067/1067 [==============================] - 13s 12ms/step - loss: 4.0842 - cls_loss: 0.1806 - reg_loss: 0.0054 - intention_accuracy: 0.5956 - val_loss: 3.2544 - va
l_cls_loss: 0.3688 - val_reg_loss: 0.0029 - val_intention_accuracy: 0.6983                                                                                          [Sigma] Epoch 2: sigma_cls=0.8023 sigma_reg=1.5377
Epoch 3/30
1067/1067 [==============================] - 13s 12ms/step - loss: 2.4108 - cls_loss: 0.1713 - reg_loss: 0.0041 - intention_accuracy: 0.6275 - val_loss: 2.2290 - va
l_cls_loss: 0.3066 - val_reg_loss: 0.0031 - val_intention_accuracy: 0.6653                                                                                          [Sigma] Epoch 3: sigma_cls=0.7661 sigma_reg=1.4829
Epoch 4/30
1067/1067 [==============================] - 13s 12ms/step - loss: 1.7041 - cls_loss: 0.1627 - reg_loss: 0.0032 - intention_accuracy: 0.6401 - val_loss: 1.7348 - va
l_cls_loss: 0.3090 - val_reg_loss: 0.0035 - val_intention_accuracy: 0.5579                                                                                          [Sigma] Epoch 4: sigma_cls=0.7326 sigma_reg=1.4279
Epoch 5/30
1067/1067 [==============================] - 14s 13ms/step - loss: 1.2563 - cls_loss: 0.1531 - reg_loss: 0.0032 - intention_accuracy: 0.6781 - val_loss: 1.4403 - va
l_cls_loss: 0.3258 - val_reg_loss: 0.0033 - val_intention_accuracy: 0.5826                                                                                          [Sigma] Epoch 5: sigma_cls=0.7018 sigma_reg=1.3727
Epoch 6/30
1067/1067 [==============================] - 18s 17ms/step - loss: 0.9487 - cls_loss: 0.1487 - reg_loss: 0.0032 - intention_accuracy: 0.6973 - val_loss: 1.4472 - va
l_cls_loss: 0.4289 - val_reg_loss: 0.0036 - val_intention_accuracy: 0.6446                                                                                          [Sigma] Epoch 6: sigma_cls=0.6743 sigma_reg=1.3175
Epoch 7/30
1067/1067 [==============================] - 24s 22ms/step - loss: 0.7140 - cls_loss: 0.1397 - reg_loss: 0.0030 - intention_accuracy: 0.6992 - val_loss: 1.2567 - va
l_cls_loss: 0.4044 - val_reg_loss: 0.0042 - val_intention_accuracy: 0.5455                                                                                          [Sigma] Epoch 7: sigma_cls=0.6481 sigma_reg=1.2622
Epoch 8/30
1067/1067 [==============================] - 24s 23ms/step - loss: 0.5285 - cls_loss: 0.1305 - reg_loss: 0.0031 - intention_accuracy: 0.7099 - val_loss: 1.3500 - va
l_cls_loss: 0.4771 - val_reg_loss: 0.0037 - val_intention_accuracy: 0.5537                                                                                          [Sigma] Epoch 8: sigma_cls=0.6236 sigma_reg=1.2067
Epoch 9/30
1067/1067 [==============================] - 24s 22ms/step - loss: 0.3835 - cls_loss: 0.1255 - reg_loss: 0.0031 - intention_accuracy: 0.7245 - val_loss: 1.3742 - va
l_cls_loss: 0.5044 - val_reg_loss: 0.0036 - val_intention_accuracy: 0.6116                                                                                          [Sigma] Epoch 9: sigma_cls=0.6010 sigma_reg=1.1512
Epoch 10/30
1067/1067 [==============================] - 24s 22ms/step - loss: 0.2529 - cls_loss: 0.1191 - reg_loss: 0.0029 - intention_accuracy: 0.7582 - val_loss: 1.2153 - va
l_cls_loss: 0.4596 - val_reg_loss: 0.0039 - val_intention_accuracy: 0.5331                                                                                          [Sigma] Epoch 10: sigma_cls=0.5798 sigma_reg=1.0956
Epoch 11/30
1067/1067 [==============================] - 24s 22ms/step - loss: 0.1558 - cls_loss: 0.1193 - reg_loss: 0.0026 - intention_accuracy: 0.7709 - val_loss: 1.1090 - va
l_cls_loss: 0.4348 - val_reg_loss: 0.0032 - val_intention_accuracy: 0.6777                                                                                          [Sigma] Epoch 11: sigma_cls=0.5623 sigma_reg=1.0398
Epoch 12/30
1067/1067 [==============================] - 24s 22ms/step - loss: 0.0209 - cls_loss: 0.1050 - reg_loss: 0.0025 - intention_accuracy: 0.7952 - val_loss: 1.2467 - va
l_cls_loss: 0.4796 - val_reg_loss: 0.0042 - val_intention_accuracy: 0.6157                                                                                          [Sigma] Epoch 12: sigma_cls=0.5431 sigma_reg=0.9839
Epoch 13/30
1067/1067 [==============================] - 24s 22ms/step - loss: -0.0836 - cls_loss: 0.1012 - reg_loss: 0.0022 - intention_accuracy: 0.8172 - val_loss: 1.7708 - v
al_cls_loss: 0.6271 - val_reg_loss: 0.0029 - val_intention_accuracy: 0.5868                                                                                         [Sigma] Epoch 13: sigma_cls=0.5264 sigma_reg=0.9279
Epoch 14/30
1067/1067 [==============================] - 24s 22ms/step - loss: -0.1846 - cls_loss: 0.0971 - reg_loss: 0.0021 - intention_accuracy: 0.8257 - val_loss: 1.7620 - v
al_cls_loss: 0.6181 - val_reg_loss: 0.0022 - val_intention_accuracy: 0.5868                                                                                         [Sigma] Epoch 14: sigma_cls=0.5116 sigma_reg=0.8717
Epoch 15/30
 949/1067 [=========================>....] - ETA: 2s - loss: -0.2752 - cls_loss: 0.0950 - reg_loss: 0 952/1067 [=========================>....] - ETA: 2s - loss: -0
.2760 - cls_loss: 0.0949 - reg_loss: 0 955/1067 [=========================>....] - ETA: 2s - loss: -0.2750 - cls_loss: 0.0952 - reg_loss: 0 958/1067 [=========================>....] - ETA: 2s - loss: -0.2748 - cls_loss: 0.0952 - reg_loss: 0 960/1067 [=========================>....] - ETA: 2s - loss: -0.2752 - cls_loss: 0.0951 - reg_loss: 0 963/1067 [==========================>...] - ETA: 2s - loss: -0.2754 - cls_loss: 0.0951 - reg_loss: 0 966/1067 [==========================>...] - ETA: 2s - loss: -0.2744 - cls_loss: 0.0954 - reg_loss: 0 969/1067 [==========================>...] - ETA: 2s - loss: -0.2740 - cls_loss: 0.0955 - reg_loss: 0 972/1067 [==========================>...] - ETA: 2s - loss: -0.2747 - cls_loss: 0.0954 - reg_loss: 0 975/1067 [==========================>...] - ETA: 1s - loss: -0.2748 - cls_loss: 0.0954 - reg_loss: 0 978/1067 [==========================>...] - ETA: 1s - loss: -0.2758 - cls_loss: 0.0952 - reg_loss: 0 981/1067 [==========================>...] - ETA: 1s - loss: -0.2763 - cls_loss: 0.0951 - reg_loss: 0 984/1067 [==========================>...] - ETA: 1s - loss: -0.2774 - cls_loss: 0.0948 - reg_loss: 0 987/1067 [==========================>...] - ETA: 1s - loss: -0.2772 - cls_loss: 0.0949 - reg_loss: 0 989/1067 [==========================>...] - ETA: 1s - loss: -0.2780 - cls_loss: 0.0948 - reg_loss: 0 991/1067 [==========================>...] - ETA: 1s - loss: -0.2782 - cls_loss: 0.0947 - reg_loss: 0 993/1067 [==========================>...] - ETA: 1s - loss: -0.2784 - cls_loss: 0.0947 - reg_loss: 0 995/1067 [==========================>...] - ETA: 1s - loss: -0.2788 - cls_loss: 0.0946 - reg_loss: 0 998/1067 [===========================>..] - ETA: 1s - loss: -0.2790 - cls_loss: 0.0946 - reg_loss: 01001/1067 [===========================>..] - ETA: 1s - loss: -0.2781 - cls_loss: 0.0948 - reg_loss: 01004/1067 [===========================>..] - ETA: 1s - loss: -0.2779 - cls_loss: 0.0949 - reg_loss: 01008/1067 [===========================>..] - ETA: 1s - loss: -0.2786 - cls_loss: 0.0948 - reg_loss: 01011/1067 [===========================>..] - ETA: 1s - loss: -0.2797 - cls_loss: 0.0946 - reg_loss: 01014/1067 [===========================>..] - ETA: 1s - loss: -0.2788 - cls_loss: 0.0948 - reg_loss: 01017/1067 [===========================>..] - ETA: 1s - loss: -0.2790 - cls_loss: 0.0948 - reg_loss: 01020/1067 [===========================>..] - ETA: 0s - loss: -0.2798 - cls_loss: 0.0946 - reg_loss: 01023/1067 [===========================>..] - ETA: 0s - loss: -0.2802 - cls_loss: 0.0946 - reg_loss: 01027/1067 [===========================>..] - ETA: 0s - loss: -0.2814 - cls_loss: 0.0943 - reg_loss: 01030/1067 [===========================>..] - ETA: 0s - loss: -0.2813 - cls_loss: 0.0944 - reg_loss: 01033/1067 [============================>.] - ETA: 0s - loss: -0.2814 - cls_loss: 0.0944 - reg_loss: 01036/1067 [============================>.] - ETA: 0s - loss: -0.2826 - cls_loss: 0.0941 - reg_loss: 01038/1067 [============================>.] - ETA: 0s - loss: -0.2823 - cls_loss: 0.0942 - reg_loss: 01041/1067 [============================>.] - ETA: 0s - loss: -0.2819 - cls_loss: 0.0943 - reg_loss: 01045/1067 [============================>.] - ETA: 0s - loss: -0.2835 - cls_loss: 0.0940 - reg_loss: 01048/1067 [============================>.] - ETA: 0s - loss: -0.2797 - cls_loss: 0.0949 - reg_loss: 01052/1067 [============================>.] - ETA: 0s - loss: -0.2805 - cls_loss: 0.0948 - reg_loss: 01055/1067 [============================>.] - ETA: 0s - loss: -0.2811 - cls_loss: 0.0947 - reg_loss: 01058/1067 [============================>.] - ETA: 0s - loss: -0.2795 - cls_loss: 0.0951 - reg_loss: 01061/1067 [============================>.] - ETA: 0s - loss: -0.2797 - cls_loss: 0.0951 - reg_loss: 01064/1067 [============================>.] - ETA: 0s - loss: -0.2806 - cls_loss: 0.0949 - reg_loss: 01067/1067 [==============================] - ETA: 0s - loss: -0.2815 - cls_loss: 0.0947 - reg_loss: 01067/1067 [==============================] - 24s 22ms/step - loss: -0.2815 - cls_loss: 0.0947 - reg_loss: 0.0019 - intention_accuracy: 0.8299 - val_loss: 1.4322 - val_cls_loss: 0.5316 - val_reg_loss: 0.0024 - val_intention_accuracy: 0.6116                                                [Sigma] Epoch 15: sigma_cls=0.4985 sigma_reg=0.8153
Epoch 16/30
   1/1067 [..............................] - ETA: 37s - loss: -0.4551 - cls_loss: 0.0630 - reg_loss: 
   4/1067 [..............................] - ETA: 22s - loss: -0.3286 - cls_loss: 0.0945 - reg_loss:                                                                   7/1067 [..............................] - ETA: 20s - loss: -0.2694 - cls_loss: 0.1093 - reg_loss:                                                                  10/1067 [..............................] - ETA: 20s - loss: -0.3467 - cls_loss: 0.0900 - reg_loss:                                                                  13/1067 [..............................] - ETA: 19s - loss: -0.3673 - cls_loss: 0.0849 - reg_loss:                                                                  16/1067 [..............................] - ETA: 20s - loss: -0.4088 - cls_loss: 0.0747 - reg_loss:                                                                  19/1067 [..............................] - ETA: 19s - loss: -0.4218 - cls_loss: 0.0715 - reg_loss:                                                                  22/1067 [..............................] - ETA: 19s - loss: -0.4318 - cls_loss: 0.0690 - reg_loss:                                                                  25/1067 [..............................] - ETA: 19s - loss: -0.4294 - cls_loss: 0.0697 - reg_loss:                                                                  28/1067 [..............................] - ETA: 19s - loss: -0.4316 - cls_loss: 0.0692 - reg_loss:                                                                  31/1067 [..............................] - ETA: 19s - loss: -0.4472 - cls_loss: 0.0654 - reg_loss:                                                                  34/1067 [..............................] - ETA: 19s - loss: -0.4348 - cls_loss: 0.0685 - reg_loss:                                                                  37/1067 [>.............................] - ETA: 19s - loss: -0.3776 - cls_loss: 0.0827 - reg_loss:                                                                  40/1067 [>.............................] - ETA: 19s - loss: -0.3882 - cls_loss: 0.0801 - reg_loss:                                                                  43/1067 [>.............................] - ETA: 19s - loss: -0.3833 - cls_loss: 0.0813 - reg_loss:                                                                  47/1067 [>.............................] - ETA: 19s - loss: -0.3839 - cls_loss: 0.0812 - reg_loss:                                                                  51/1067 [>.............................] - ETA: 18s - loss: -0.3613 - cls_loss: 0.0868 - reg_loss:                                                                  54/1067 [>.............................] - ETA: 18s - loss: -0.3492 - cls_loss: 0.0899 - reg_loss:                                                                  58/1067 [>.............................] - ETA: 18s - loss: -0.3240 - cls_loss: 0.0962 - reg_loss:                                                                  60/1067 [>.............................] - ETA: 19s - loss: -0.3130 - cls_loss: 0.0989 - reg_loss:                                                                  63/1067 [>.............................] - ETA: 19s - loss: -0.3137 - cls_loss: 0.0988 - reg_loss:                                                                  67/1067 [>.............................] - ETA: 18s - loss: -0.3104 - cls_loss: 0.0996 - reg_loss:                                                                  70/1067 [>.............................] - ETA: 18s - loss: -0.3183 - cls_loss: 0.0977 - reg_loss:                                                                  73/1067 [=>............................] - ETA: 18s - loss: -0.3209 - cls_loss: 0.0971 - reg_loss:                                                                  76/1067 [=>............................] - ETA: 18s - loss: -0.3273 - cls_loss: 0.0955 - reg_loss:                                                                  79/1067 [=>............................] - ETA: 18s - loss: -0.3232 - cls_loss: 0.0966 - reg_loss:                                                                  82/1067 [=>............................] - ETA: 18s - loss: -0.3078 - cls_loss: 0.1004 - reg_loss:                                                                  85/1067 [=>............................] - ETA: 18s - loss: -0.3182 - cls_loss: 0.0979 - reg_loss:                                                                  88/1067 [=>............................] - ETA: 18s - loss: -0.3150 - cls_loss: 0.0987 - reg_loss:                                                                  91/1067 [=>............................] - ETA: 19s - loss: -0.3160 - cls_loss: 0.0985 - reg_loss:                                                                  95/1067 [=>............................] - ETA: 18s - loss: -0.3177 - cls_loss: 0.0981 - reg_loss:                                                                  97/1067 [=>............................] - ETA: 19s - loss: -0.3195 - cls_loss: 0.0977 - reg_loss:                                                                 100/1067 [=>............................] - ETA: 18s - loss: -0.3291 - cls_loss: 0.0954 - reg_loss:                                                                 102/1067 [=>............................] - ETA: 19s - loss: -0.3319 - cls_loss: 0.0946 - reg_loss:                                                                 104/1067 [=>............................] - ETA: 19s - loss: -0.3315 - cls_loss: 0.0948 - reg_loss:                                                                 107/1067 [==>...........................] - ETA: 19s - loss: -0.3412 - cls_loss: 0.0924 - reg_loss:                                                                 110/1067 [==>...........................] - ETA: 19s - loss: -0.3471 - cls_loss: 0.0909 - reg_loss:                                                                 113/1067 [==>...........................] - ETA: 19s - loss: -0.3332 - cls_loss: 0.0944 - reg_loss:                                                                 116/1067 [==>...........................] - ETA: 19s - loss: -0.3378 - cls_loss: 0.0933 - reg_loss:                                                                1067/1067 [==============================] - 24s 22ms/step - loss: -0.4177 - cls_loss: 0.0836 - reg_loss: 0.0018 - intention_accuracy: 0.8388 - val_loss: 2.4499 - val_cls_loss: 0.7627 - val_reg_loss: 0.0031 - val_intention_accuracy: 0.6157                                                                                         [Sigma] Epoch 16: sigma_cls=0.4829 sigma_reg=0.7587
Epoch 17/30
1067/1067 [==============================] - 23s 21ms/step - loss: -0.5209 - cls_loss: 0.0816 - reg_loss: 0.0018 - intention_accuracy: 0.8515 - val_loss: 1.9448 - v
al_cls_loss: 0.6334 - val_reg_loss: 0.0021 - val_intention_accuracy: 0.6570                                                                                         [Sigma] Epoch 17: sigma_cls=0.4686 sigma_reg=0.7019
Epoch 18/30
1067/1067 [==============================] - 23s 21ms/step - loss: -0.6013 - cls_loss: 0.0852 - reg_loss: 0.0018 - intention_accuracy: 0.8566 - val_loss: 1.4690 - v
al_cls_loss: 0.5312 - val_reg_loss: 0.0016 - val_intention_accuracy: 0.5702                                                                                         [Sigma] Epoch 18: sigma_cls=0.4587 sigma_reg=0.6448
Epoch 19/30
1067/1067 [==============================] - 23s 22ms/step - loss: -0.7415 - cls_loss: 0.0772 - reg_loss: 0.0016 - intention_accuracy: 0.8571 - val_loss: 2.4026 - v
al_cls_loss: 0.7195 - val_reg_loss: 0.0023 - val_intention_accuracy: 0.5868                                                                                         [Sigma] Epoch 19: sigma_cls=0.4483 sigma_reg=0.5874
Epoch 20/30
1067/1067 [==============================] - 22s 21ms/step - loss: -0.8607 - cls_loss: 0.0755 - reg_loss: 0.0015 - intention_accuracy: 0.8664 - val_loss: 2.1291 - v
al_cls_loss: 0.6612 - val_reg_loss: 0.0016 - val_intention_accuracy: 0.6322                                                                                         [Sigma] Epoch 20: sigma_cls=0.4383 sigma_reg=0.5296
Epoch 21/30
1067/1067 [==============================] - 24s 22ms/step - loss: -0.9563 - cls_loss: 0.0799 - reg_loss: 0.0015 - intention_accuracy: 0.8486 - val_loss: 1.5958 - v
al_cls_loss: 0.5667 - val_reg_loss: 0.0017 - val_intention_accuracy: 0.6736                                                                                         [Sigma] Epoch 21: sigma_cls=0.4316 sigma_reg=0.4713
Epoch 22/30
1067/1067 [==============================] - 23s 22ms/step - loss: -1.1305 - cls_loss: 0.0721 - reg_loss: 0.0014 - intention_accuracy: 0.8669 - val_loss: 1.9747 - v
al_cls_loss: 0.6394 - val_reg_loss: 0.0015 - val_intention_accuracy: 0.6033                                                                                         [Sigma] Epoch 22: sigma_cls=0.4225 sigma_reg=0.4124
Epoch 23/30
1067/1067 [==============================] - 23s 21ms/step - loss: -1.2843 - cls_loss: 0.0716 - reg_loss: 0.0014 - intention_accuracy: 0.8735 - val_loss: 1.6362 - v
al_cls_loss: 0.5883 - val_reg_loss: 0.0018 - val_intention_accuracy: 0.6281                                                                                         [Sigma] Epoch 23: sigma_cls=0.4150 sigma_reg=0.3527
Epoch 24/30
1067/1067 [==============================] - 23s 22ms/step - loss: -1.4482 - cls_loss: 0.0736 - reg_loss: 0.0014 - intention_accuracy: 0.8707 - val_loss: 1.0146 - v
al_cls_loss: 0.5027 - val_reg_loss: 0.0019 - val_intention_accuracy: 0.6198                                                                                         [Sigma] Epoch 24: sigma_cls=0.4098 sigma_reg=0.2921
Epoch 25/30
1067/1067 [==============================] - 24s 22ms/step - loss: -1.7018 - cls_loss: 0.0666 - reg_loss: 0.0013 - intention_accuracy: 0.8707 - val_loss: 1.6588 - v
al_cls_loss: 0.6275 - val_reg_loss: 0.0018 - val_intention_accuracy: 0.5702                                                                                         [Sigma] Epoch 25: sigma_cls=0.4020 sigma_reg=0.2301
Epoch 26/30
1067/1067 [==============================] - 23s 22ms/step - loss: -1.9531 - cls_loss: 0.0688 - reg_loss: 0.0014 - intention_accuracy: 0.8716 - val_loss: 2.1271 - v
al_cls_loss: 0.7385 - val_reg_loss: 0.0011 - val_intention_accuracy: 0.5950                                                                                         [Sigma] Epoch 26: sigma_cls=0.3967 sigma_reg=0.1662
Epoch 27/30
1067/1067 [==============================] - 23s 22ms/step - loss: -2.3378 - cls_loss: 0.0663 - reg_loss: 0.0012 - intention_accuracy: 0.8805 - val_loss: 1.6584 - v
al_cls_loss: 0.7146 - val_reg_loss: 0.0011 - val_intention_accuracy: 0.6488                                                                                         [Sigma] Epoch 27: sigma_cls=0.3914 sigma_reg=0.1014
Epoch 28/30
1067/1067 [==============================] - 19s 18ms/step - loss: -2.8048 - cls_loss: 0.0612 - reg_loss: 0.0012 - intention_accuracy: 0.8847 - val_loss: 2.4318 - v
al_cls_loss: 0.8604 - val_reg_loss: 0.0011 - val_intention_accuracy: 0.6240                                                                                         [Sigma] Epoch 28: sigma_cls=0.3849 sigma_reg=0.0568
Epoch 29/30
1067/1067 [==============================] - 14s 13ms/step - loss: -2.9830 - cls_loss: 0.0655 - reg_loss: 0.0010 - intention_accuracy: 0.8786 - val_loss: 2.0368 - v
al_cls_loss: 0.8118 - val_reg_loss: 7.8260e-04 - val_intention_accuracy: 0.6281                                                                                     [Sigma] Epoch 29: sigma_cls=0.3809 sigma_reg=0.0473
Epoch 30/30
1067/1067 [==============================] - 15s 14ms/step - loss: -3.0944 - cls_loss: 0.0605 - reg_loss: 9.0969e-04 - intention_accuracy: 0.8941 - val_loss: 2.6404
 - val_cls_loss: 0.8807 - val_reg_loss: 7.6587e-04 - val_intention_accuracy: 0.6364                                                                                 [Sigma] Epoch 30: sigma_cls=0.3754 sigma_reg=0.0428

ğŸ¯ Training completed!
ğŸ“ All epoch models saved in: data/models/jaad/Transformer_depth/12Nov2025-05h36m22s/epochs
Train model is saved to data/models/jaad/Transformer_depth/12Nov2025-05h36m22s/model.h5
Available metrics: ['loss', 'cls_loss', 'reg_loss', 'intention_accuracy', 'val_loss', 'val_cls_loss', 'val_reg_loss', 'val_intention_accuracy', 'sigma_cls', 'val_si
gma_cls', 'sigma_reg', 'val_sigma_reg']                                                                                                                             Training plots saved to model directory
Wrote configs to data/models/jaad/Transformer_depth/12Nov2025-05h36m22s/configs.yaml
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 15s 7ms/step

======================================================================
ğŸ¯ MODEL TEST RESULTS ğŸ¯
======================================================================
Accuracy:   0.6247
AUC:        0.5577
F1-Score:   0.7332
Precision:  0.6603
Recall:     0.8241
======================================================================

Model saved to data/models/jaad/Transformer_depth/12Nov2025-05h36m22s/

âœ… è®­ç»ƒå®Œæˆ (è€—æ—¶: 11.4 åˆ†é’Ÿ)
ğŸ” æŸ¥æ‰¾æœ€æ–°æ¨¡å‹ç›®å½•...
ğŸ“ æ‰¾åˆ°æ¨¡å‹ç›®å½•: data/models/jaad/Transformer_depth/12Nov2025-05h36m22s

ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å‹...
2025-11-12 05:47:43.029598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:47:43.036196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:47:43.036391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             ğŸš€ å¼€å§‹æµ‹è¯•æ¨¡å‹ç›®å½•: data/models/jaad/Transformer_depth/12Nov2025-05h36m22s
âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ
âœ… æ•°æ®é›†åˆå§‹åŒ–æˆåŠŸ
ğŸ”„ ç”Ÿæˆæµ‹è¯•æ•°æ®...
---------------------------------------------------------
Generating action sequence data
fstride: 1
sample_type: beh
subset: default
height_rng: [0, inf]
squarify_ratio: 0
data_split_type: default
seq_type: crossing
min_track_size: 76
random_params: {'ratios': None, 'val_data': True, 'regen_data': False}
kfold_params: {'num_folds': 5, 'fold': 1}
---------------------------------------------------------
Generating database for jaad
jaad database loaded from /home/minshi/Pedestrian_Crossing_Intention_Prediction/JAAD/data_cache/jaad_database.pkl
---------------------------------------------------------
Generating crossing data
Split: test
Number of pedestrians: 276 
Total number of samples: 171 
âœ… æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ
ğŸ“ æ‰¾åˆ° 31 ä¸ªæ¨¡å‹æ–‡ä»¶

============================================================
è¿›åº¦: 1/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_001_loss_5.9273_acc_0.5579.h5
2025-11-12 05:47:44.567749: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN
) to use the following CPU instructions in performance-critical operations:  AVX2 FMA                                                                               To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-12 05:47:44.571063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:47:44.571284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:47:44.571420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:47:45.122902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:47:45.123124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:47:45.123268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must
 be at least one NUMA node, so returning NUMA node zero                                                                                                             2025-11-12 05:47:45.123446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3498 MB memory
:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9                                                        [DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
2025-11-12 05:47:47.034949: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-11-12 05:47:48.998881: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged
 once.                                                                                                                                                              2025-11-12 05:47:49.630201: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100
1881/1881 [==============================] - 21s 10ms/step
âœ… å‡†ç¡®ç‡: 0.6374, AUC: 0.7185, F1: 0.6801

============================================================
è¿›åº¦: 2/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_002_loss_3.2544_acc_0.6983.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 9ms/step
âœ… å‡†ç¡®ç‡: 0.6582, AUC: 0.7248, F1: 0.7239

============================================================
è¿›åº¦: 3/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_003_loss_2.2290_acc_0.6653.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®ç‡: 0.6608, AUC: 0.7032, F1: 0.7073

============================================================
è¿›åº¦: 4/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_004_loss_1.7348_acc_0.5579.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 10ms/step
âœ… å‡†ç¡®ç‡: 0.6385, AUC: 0.7070, F1: 0.6562

============================================================
è¿›åº¦: 5/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_005_loss_1.4403_acc_0.5826.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 18s 9ms/step
âœ… å‡†ç¡®ç‡: 0.6667, AUC: 0.7068, F1: 0.7237

============================================================
è¿›åº¦: 6/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_006_loss_1.4472_acc_0.6446.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 11s 5ms/step
âœ… å‡†ç¡®ç‡: 0.6438, AUC: 0.6973, F1: 0.6946

============================================================
è¿›åº¦: 7/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_007_loss_1.2567_acc_0.5455.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 11s 5ms/step
âœ… å‡†ç¡®ç‡: 0.6507, AUC: 0.7008, F1: 0.6794

============================================================
è¿›åº¦: 8/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_008_loss_1.3500_acc_0.5537.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 13s 7ms/step
âœ… å‡†ç¡®ç‡: 0.6842, AUC: 0.6987, F1: 0.7519

============================================================
è¿›åº¦: 9/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_009_loss_1.3742_acc_0.6116.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 18s 9ms/step
âœ… å‡†ç¡®ç‡: 0.6566, AUC: 0.6821, F1: 0.7131

============================================================
è¿›åº¦: 10/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_010_loss_1.2153_acc_0.5331.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 10ms/step
âœ… å‡†ç¡®ç‡: 0.6470, AUC: 0.6855, F1: 0.6987

============================================================
è¿›åº¦: 11/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_011_loss_1.1090_acc_0.6777.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®ç‡: 0.6358, AUC: 0.6692, F1: 0.7196

============================================================
è¿›åº¦: 12/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_012_loss_1.2467_acc_0.6157.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 9ms/step
âœ… å‡†ç¡®ç‡: 0.6640, AUC: 0.6715, F1: 0.7486

============================================================
è¿›åº¦: 13/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_013_loss_1.7708_acc_0.5868.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®ç‡: 0.6039, AUC: 0.6427, F1: 0.6958

============================================================
è¿›åº¦: 14/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_014_loss_1.7620_acc_0.5868.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 10ms/step
âœ… å‡†ç¡®ç‡: 0.6449, AUC: 0.6808, F1: 0.7246

============================================================
è¿›åº¦: 15/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_015_loss_1.4322_acc_0.6116.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®ç‡: 0.6247, AUC: 0.6584, F1: 0.7044

============================================================
è¿›åº¦: 16/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_016_loss_2.4499_acc_0.6157.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 9ms/step
âœ… å‡†ç¡®ç‡: 0.6768, AUC: 0.6903, F1: 0.7516

============================================================
è¿›åº¦: 17/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_017_loss_1.9448_acc_0.6570.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 9ms/step
âœ… å‡†ç¡®ç‡: 0.6385, AUC: 0.6771, F1: 0.7178

============================================================
è¿›åº¦: 18/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_018_loss_1.4690_acc_0.5702.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 18s 9ms/step
âœ… å‡†ç¡®ç‡: 0.6497, AUC: 0.6584, F1: 0.7232

============================================================
è¿›åº¦: 19/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_019_loss_2.4026_acc_0.5868.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 16s 8ms/step
âœ… å‡†ç¡®ç‡: 0.6502, AUC: 0.6759, F1: 0.7321

============================================================
è¿›åº¦: 20/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_020_loss_2.1291_acc_0.6322.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 11s 6ms/step
âœ… å‡†ç¡®ç‡: 0.5965, AUC: 0.6410, F1: 0.7039

============================================================
è¿›åº¦: 21/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_021_loss_1.5958_acc_0.6736.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 11s 5ms/step
âœ… å‡†ç¡®ç‡: 0.6247, AUC: 0.6589, F1: 0.7303

============================================================
è¿›åº¦: 22/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_022_loss_1.9747_acc_0.6033.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 11s 6ms/step
âœ… å‡†ç¡®ç‡: 0.6289, AUC: 0.6734, F1: 0.7153

============================================================
è¿›åº¦: 23/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_023_loss_1.6362_acc_0.6281.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 14s 7ms/step
âœ… å‡†ç¡®ç‡: 0.6225, AUC: 0.6556, F1: 0.7130

============================================================
è¿›åº¦: 24/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_024_loss_1.0146_acc_0.6198.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 9ms/step
âœ… å‡†ç¡®ç‡: 0.6124, AUC: 0.6429, F1: 0.6915

============================================================
è¿›åº¦: 25/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_025_loss_1.6588_acc_0.5702.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 15s 7ms/step
âœ… å‡†ç¡®ç‡: 0.6316, AUC: 0.6714, F1: 0.7156

============================================================
è¿›åº¦: 26/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_026_loss_2.1271_acc_0.5950.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 10s 5ms/step
âœ… å‡†ç¡®ç‡: 0.6321, AUC: 0.6647, F1: 0.7265

============================================================
è¿›åº¦: 27/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_027_loss_1.6584_acc_0.6488.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 10s 5ms/step
âœ… å‡†ç¡®ç‡: 0.6077, AUC: 0.6555, F1: 0.7209

============================================================
è¿›åº¦: 28/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_028_loss_2.4318_acc_0.6240.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 10s 5ms/step
âœ… å‡†ç¡®ç‡: 0.6390, AUC: 0.6616, F1: 0.7309

============================================================
è¿›åº¦: 29/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_029_loss_2.0368_acc_0.6281.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 11s 6ms/step
âœ… å‡†ç¡®ç‡: 0.6140, AUC: 0.6558, F1: 0.7164

============================================================
è¿›åº¦: 30/31

ğŸ” æµ‹è¯•æ¨¡å‹: epoch_030_loss_2.6404_acc_0.6364.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 13s 7ms/step
âœ… å‡†ç¡®ç‡: 0.6247, AUC: 0.6513, F1: 0.7332

============================================================
è¿›åº¦: 31/31

ğŸ” æµ‹è¯•æ¨¡å‹: model.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®ç‡: 0.6247, AUC: 0.6513, F1: 0.7332

ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: data/models/jaad/Transformer_depth/12Nov2025-05h36m22s/test_results_20251112_055648.csv
ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: data/models/jaad/Transformer_depth/12Nov2025-05h36m22s/test_report_20251112_055648.txt

ğŸ† å‡†ç¡®ç‡æœ€é«˜çš„æ¨¡å‹: epoch_008_loss_1.3500_acc_0.5537.h5 (å‡†ç¡®ç‡: 0.6842)
ğŸ—‘ï¸  å¼€å§‹æ¸…ç†epochsç›®å½•ï¼Œå°†åˆ é™¤ 30 ä¸ªæ¨¡å‹æ–‡ä»¶...
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_016_loss_2.4499_acc_0.6157.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_001_loss_5.9273_acc_0.5579.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_005_loss_1.4403_acc_0.5826.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_011_loss_1.1090_acc_0.6777.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_022_loss_1.9747_acc_0.6033.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_009_loss_1.3742_acc_0.6116.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_007_loss_1.2567_acc_0.5455.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_024_loss_1.0146_acc_0.6198.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_020_loss_2.1291_acc_0.6322.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_006_loss_1.4472_acc_0.6446.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_014_loss_1.7620_acc_0.5868.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_013_loss_1.7708_acc_0.5868.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_028_loss_2.4318_acc_0.6240.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_025_loss_1.6588_acc_0.5702.h5
ğŸ“‹ å·²å°†æœ€ä½³æ¨¡å‹å¤åˆ¶åˆ°: data/models/jaad/Transformer_depth/12Nov2025-05h36m22s/epoch_008_loss_1.3500_acc_0.5537.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_008_loss_1.3500_acc_0.5537.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_002_loss_3.2544_acc_0.6983.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_029_loss_2.0368_acc_0.6281.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_015_loss_1.4322_acc_0.6116.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_004_loss_1.7348_acc_0.5579.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_021_loss_1.5958_acc_0.6736.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_030_loss_2.6404_acc_0.6364.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_017_loss_1.9448_acc_0.6570.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_019_loss_2.4026_acc_0.5868.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_027_loss_1.6584_acc_0.6488.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_003_loss_2.2290_acc_0.6653.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_023_loss_1.6362_acc_0.6281.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_026_loss_2.1271_acc_0.5950.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_012_loss_1.2467_acc_0.6157.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_010_loss_1.2153_acc_0.5331.h5
ğŸ—‘ï¸  å·²åˆ é™¤: epoch_018_loss_1.4690_acc_0.5702.h5
ğŸ—‘ï¸  å·²åˆ é™¤ç©ºçš„epochsç›®å½•
ğŸ”„ æ¨¡å‹ç›®å½•å·²é‡å‘½å:
   åŸç›®å½•: 12Nov2025-05h36m22s
   æ–°ç›®å½•: 12Nov2025-05h36m22s_acc_0.6842

================================================================================
ğŸ¯ æµ‹è¯•ç»“æœæ±‡æ€»
================================================================================
æ€»æ¨¡å‹æ•°é‡: 31
æˆåŠŸæµ‹è¯•: 31
å¤±è´¥æµ‹è¯•: 0

ğŸ“Š æ€§èƒ½ç»Ÿè®¡:
å¹³å‡å‡†ç¡®ç‡: 0.6384 (Â±0.0209)
å¹³å‡AUC: 0.6752 (Â±0.0231)
å¹³å‡F1: 0.7154 (Â±0.0213)

ğŸ† æœ€ä½³æ¨¡å‹:
æœ€é«˜å‡†ç¡®ç‡: epoch_008_loss_1.3500_acc_0.5537.h5 (Acc: 0.6842)
æœ€é«˜AUC: epoch_002_loss_3.2544_acc_0.6983.h5 (AUC: 0.7248)
æœ€é«˜F1: epoch_008_loss_1.3500_acc_0.5537.h5 (F1: 0.7519)

âœ… æµ‹è¯•å®Œæˆ (è€—æ—¶: 9.2 åˆ†é’Ÿ)

================================================================================
ğŸ‰ è®­ç»ƒå’Œæµ‹è¯•ç®¡é“å®Œæˆ!
================================================================================
æ¨¡å‹ç›®å½•: data/models/jaad/Transformer_depth/12Nov2025-05h36m22s
æ€»è€—æ—¶: 20.6 åˆ†é’Ÿ
ç»“æŸæ—¶é—´: 2025-11-12 05:56:49
================================================================================
