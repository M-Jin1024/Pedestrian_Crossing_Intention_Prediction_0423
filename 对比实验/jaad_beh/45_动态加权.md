Epoch 23/30
1067/1067 [==============================] - 23s 22ms/step - loss: -1.6374 - cls_loss: 0.0683 - reg_loss: 0.0012 - intention_accuracy: 0.8768 - val_loss: 0.9622 - val_cls_loss: 0.6687 - val_reg_loss: 0.0011 - val_intention_accuracy: 0.6281
[Sigma] Epoch 23: sigma_cls=0.4777 sigma_reg=0.2548
Epoch 24/30
1067/1067 [==============================] - 23s 21ms/step - loss: -1.7031 - cls_loss: 0.0671 - reg_loss: 0.0012 - intention_accuracy: 0.8805 - val_loss: 0.7888 - val_cls_loss: 0.6243 - val_reg_loss: 0.0012 - val_intention_accuracy: 0.5785
[Sigma] Epoch 24: sigma_cls=0.4702 sigma_reg=0.2430
Epoch 25/30
1067/1067 [==============================] - 24s 22ms/step - loss: -1.7638 - cls_loss: 0.0664 - reg_loss: 0.0012 - intention_accuracy: 0.8800 - val_loss: 1.4523 - val_cls_loss: 0.7627 - val_reg_loss: 0.0014 - val_intention_accuracy: 0.5992
[Sigma] Epoch 25: sigma_cls=0.4633 sigma_reg=0.2317
Epoch 26/30
1067/1067 [==============================] - 26s 24ms/step - loss: -1.8650 - cls_loss: 0.0574 - reg_loss: 0.0012 - intention_accuracy: 0.8969 - val_loss: 2.5296 - val_cls_loss: 0.9747 - val_reg_loss: 9.5143e-04 - val_intention_accuracy: 0.6612
[Sigma] Epoch 26: sigma_cls=0.4552 sigma_reg=0.2209
Epoch 27/30
1067/1067 [==============================] - 25s 24ms/step - loss: -1.8473 - cls_loss: 0.0722 - reg_loss: 0.0012 - intention_accuracy: 0.8735 - val_loss: 2.3218 - val_cls_loss: 0.9238 - val_reg_loss: 0.0011 - val_intention_accuracy: 0.6860
[Sigma] Epoch 27: sigma_cls=0.4505 sigma_reg=0.2105
Epoch 28/30
1067/1067 [==============================] - 25s 23ms/step - loss: -1.9545 - cls_loss: 0.0623 - reg_loss: 0.0011 - intention_accuracy: 0.8880 - val_loss: 2.2658 - val_cls_loss: 0.9022 - val_reg_loss: 0.0010 - val_intention_accuracy: 0.6364
[Sigma] Epoch 28: sigma_cls=0.4446 sigma_reg=0.2006
Epoch 29/30
1067/1067 [==============================] - 26s 24ms/step - loss: -2.0127 - cls_loss: 0.0619 - reg_loss: 0.0011 - intention_accuracy: 0.8918 - val_loss: 2.4912 - val_cls_loss: 0.9338 - val_reg_loss: 0.0012 - val_intention_accuracy: 0.6074
[Sigma] Epoch 29: sigma_cls=0.4387 sigma_reg=0.1911
Epoch 30/30
1067/1067 [==============================] - 25s 23ms/step - loss: -2.0572 - cls_loss: 0.0636 - reg_loss: 0.0012 - intention_accuracy: 0.8922 - val_loss: 3.0931 - val_cls_loss: 1.0373 - val_reg_loss: 9.2120e-04 - val_intention_accuracy: 0.6612
[Sigma] Epoch 30: sigma_cls=0.4336 sigma_reg=0.1820

🎯 Training completed!
📁 All epoch models saved in: data/models/jaad/Transformer_depth/12Nov2025-02h51m22s/epochs
Train model is saved to data/models/jaad/Transformer_depth/12Nov2025-02h51m22s/model.h5
Available metrics: ['loss', 'cls_loss', 'reg_loss', 'intention_accuracy', 'val_loss', 'val_cls_loss', 'val_reg_loss', 'val_intention_accuracy', 'sigma_cls', 'val_sigma_cls', 'sigma_reg', 'val_sigma_reg']
Training plots saved to model directory
Wrote configs to data/models/jaad/Transformer_depth/12Nov2025-02h51m22s/configs.yaml
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step

======================================================================
🎯 MODEL TEST RESULTS 🎯
======================================================================
Accuracy:   0.6204
AUC:        0.5329
F1-Score:   0.7439
Precision:  0.6437
Recall:     0.8811
======================================================================

Model saved to data/models/jaad/Transformer_depth/12Nov2025-02h51m22s/

✅ 训练完成 (耗时: 9.9 分钟)
🔍 查找最新模型目录...
📁 找到模型目录: data/models/jaad/Transformer_depth/12Nov2025-02h51m22s

🧪 开始测试模型...
2025-11-12 03:01:19.451330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 03:01:19.457617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 03:01:19.457812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
🚀 开始测试模型目录: data/models/jaad/Transformer_depth/12Nov2025-02h51m22s
✅ 配置文件加载成功
✅ 数据集初始化成功
🔄 生成测试数据...
---------------------------------------------------------
Generating action sequence data
fstride: 1
sample_type: beh
subset: default
height_rng: [0, inf]
squarify_ratio: 0
data_split_type: default
seq_type: crossing
min_track_size: 76
random_params: {'ratios': None, 'val_data': True, 'regen_data': False}
kfold_params: {'num_folds': 5, 'fold': 1}
---------------------------------------------------------
Generating database for jaad
jaad database loaded from /home/minshi/Pedestrian_Crossing_Intention_Prediction/JAAD/data_cache/jaad_database.pkl
---------------------------------------------------------
Generating crossing data
Split: test
Number of pedestrians: 276 
Total number of samples: 171 
✅ 测试数据生成完成
📁 找到 31 个模型文件

============================================================
进度: 1/31

🔍 测试模型: epoch_001_loss_6.0263_acc_0.5620.h5
2025-11-12 03:01:21.019460: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-12 03:01:21.022172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 03:01:21.022396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 03:01:21.022528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 03:01:21.558148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 03:01:21.558383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 03:01:21.558672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 03:01:21.558818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3765 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
2025-11-12 03:01:23.427825: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-11-12 03:01:25.375298: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2025-11-12 03:01:26.007403: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100
1881/1881 [==============================] - 21s 10ms/step
✅ 准确率: 0.6401, AUC: 0.7215, F1: 0.6799

============================================================
进度: 2/31

🔍 测试模型: epoch_002_loss_3.3176_acc_0.6818.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
✅ 准确率: 0.6576, AUC: 0.7262, F1: 0.7257

============================================================
进度: 3/31

🔍 测试模型: epoch_003_loss_2.0633_acc_0.6612.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 11ms/step
✅ 准确率: 0.6598, AUC: 0.7046, F1: 0.7067

============================================================
进度: 4/31

🔍 测试模型: epoch_004_loss_1.4235_acc_0.5661.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 23s 11ms/step
✅ 准确率: 0.6465, AUC: 0.7085, F1: 0.6667

============================================================
进度: 5/31

🔍 测试模型: epoch_005_loss_1.0385_acc_0.5950.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
✅ 准确率: 0.6592, AUC: 0.7092, F1: 0.7202

============================================================
进度: 6/31

🔍 测试模型: epoch_006_loss_0.9410_acc_0.6405.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
✅ 准确率: 0.6433, AUC: 0.6991, F1: 0.6889

============================================================
进度: 7/31

🔍 测试模型: epoch_007_loss_0.6335_acc_0.5372.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6465, AUC: 0.7079, F1: 0.6745

============================================================
进度: 8/31

🔍 测试模型: epoch_008_loss_0.5895_acc_0.6033.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
✅ 准确率: 0.6693, AUC: 0.6989, F1: 0.7453

============================================================
进度: 9/31

🔍 测试模型: epoch_009_loss_0.6648_acc_0.6322.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6300, AUC: 0.6728, F1: 0.7056

============================================================
进度: 10/31

🔍 测试模型: epoch_010_loss_0.6728_acc_0.5661.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6252, AUC: 0.6774, F1: 0.6933

============================================================
进度: 11/31

🔍 测试模型: epoch_011_loss_0.3601_acc_0.6901.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6289, AUC: 0.6488, F1: 0.7320

============================================================
进度: 12/31

🔍 测试模型: epoch_012_loss_0.5667_acc_0.5620.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 10ms/step
✅ 准确率: 0.6656, AUC: 0.6570, F1: 0.7343

============================================================
进度: 13/31

🔍 测试模型: epoch_013_loss_0.6063_acc_0.5579.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6108, AUC: 0.6400, F1: 0.6901

============================================================
进度: 14/31

🔍 测试模型: epoch_014_loss_0.5749_acc_0.5868.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6512, AUC: 0.6686, F1: 0.7230

============================================================
进度: 15/31

🔍 测试模型: epoch_015_loss_0.4151_acc_0.6694.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6417, AUC: 0.6506, F1: 0.7340

============================================================
进度: 16/31

🔍 测试模型: epoch_016_loss_1.2309_acc_0.6777.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6645, AUC: 0.7028, F1: 0.7546

============================================================
进度: 17/31

🔍 测试模型: epoch_017_loss_0.9113_acc_0.6116.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 15s 7ms/step
✅ 准确率: 0.6353, AUC: 0.6681, F1: 0.7139

============================================================
进度: 18/31

🔍 测试模型: epoch_018_loss_0.9040_acc_0.6198.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 12s 6ms/step
✅ 准确率: 0.6481, AUC: 0.6523, F1: 0.7300

============================================================
进度: 19/31

🔍 测试模型: epoch_019_loss_1.4124_acc_0.6446.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 17s 9ms/step
✅ 准确率: 0.6135, AUC: 0.6531, F1: 0.7157

============================================================
进度: 20/31

🔍 测试模型: epoch_020_loss_0.9762_acc_0.6612.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6023, AUC: 0.6250, F1: 0.7138

============================================================
进度: 21/31

🔍 测试模型: epoch_021_loss_0.6901_acc_0.6488.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6257, AUC: 0.6649, F1: 0.7288

============================================================
进度: 22/31

🔍 测试模型: epoch_022_loss_1.5996_acc_0.6529.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6438, AUC: 0.6627, F1: 0.7360

============================================================
进度: 23/31

🔍 测试模型: epoch_023_loss_0.9622_acc_0.6281.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6268, AUC: 0.6519, F1: 0.7199

============================================================
进度: 24/31

🔍 测试模型: epoch_024_loss_0.7888_acc_0.5785.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.5928, AUC: 0.6062, F1: 0.6735

============================================================
进度: 25/31

🔍 测试模型: epoch_025_loss_1.4523_acc_0.5992.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 10ms/step
✅ 准确率: 0.6167, AUC: 0.6417, F1: 0.7113

============================================================
进度: 26/31

🔍 测试模型: epoch_026_loss_2.5296_acc_0.6612.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6050, AUC: 0.6533, F1: 0.7167

============================================================
进度: 27/31

🔍 测试模型: epoch_027_loss_2.3218_acc_0.6860.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 11ms/step
✅ 准确率: 0.6225, AUC: 0.6596, F1: 0.7368

============================================================
进度: 28/31

🔍 测试模型: epoch_028_loss_2.2658_acc_0.6364.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
✅ 准确率: 0.6247, AUC: 0.6515, F1: 0.7244

============================================================
进度: 29/31

🔍 测试模型: epoch_029_loss_2.4912_acc_0.6074.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 17s 8ms/step
✅ 准确率: 0.6029, AUC: 0.6338, F1: 0.7141

============================================================
进度: 30/31

🔍 测试模型: epoch_030_loss_3.0931_acc_0.6612.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 12s 6ms/step
✅ 准确率: 0.6204, AUC: 0.6249, F1: 0.7439

============================================================
进度: 31/31

🔍 测试模型: model.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 13s 7ms/step
✅ 准确率: 0.6204, AUC: 0.6249, F1: 0.7439

📊 结果已保存到: data/models/jaad/Transformer_depth/12Nov2025-02h51m22s/test_results_20251112_031213.csv
📝 报告已保存到: data/models/jaad/Transformer_depth/12Nov2025-02h51m22s/test_report_20251112_031213.txt

🏆 准确率最高的模型: epoch_008_loss_0.5895_acc_0.6033.h5 (准确率: 0.6693)
🗑️  开始清理epochs目录，将删除 30 个模型文件...
🗑️  已删除: epoch_014_loss_0.5749_acc_0.5868.h5
🗑️  已删除: epoch_006_loss_0.9410_acc_0.6405.h5
🗑️  已删除: epoch_012_loss_0.5667_acc_0.5620.h5
🗑️  已删除: epoch_021_loss_0.6901_acc_0.6488.h5
🗑️  已删除: epoch_010_loss_0.6728_acc_0.5661.h5
🗑️  已删除: epoch_020_loss_0.9762_acc_0.6612.h5
🗑️  已删除: epoch_022_loss_1.5996_acc_0.6529.h5
🗑️  已删除: epoch_009_loss_0.6648_acc_0.6322.h5
🗑️  已删除: epoch_016_loss_1.2309_acc_0.6777.h5
🗑️  已删除: epoch_018_loss_0.9040_acc_0.6198.h5
🗑️  已删除: epoch_028_loss_2.2658_acc_0.6364.h5
🗑️  已删除: epoch_027_loss_2.3218_acc_0.6860.h5
🗑️  已删除: epoch_030_loss_3.0931_acc_0.6612.h5
🗑️  已删除: epoch_004_loss_1.4235_acc_0.5661.h5
🗑️  已删除: epoch_029_loss_2.4912_acc_0.6074.h5
🗑️  已删除: epoch_007_loss_0.6335_acc_0.5372.h5
🗑️  已删除: epoch_003_loss_2.0633_acc_0.6612.h5
🗑️  已删除: epoch_005_loss_1.0385_acc_0.5950.h5
🗑️  已删除: epoch_015_loss_0.4151_acc_0.6694.h5
🗑️  已删除: epoch_001_loss_6.0263_acc_0.5620.h5
🗑️  已删除: epoch_011_loss_0.3601_acc_0.6901.h5
🗑️  已删除: epoch_026_loss_2.5296_acc_0.6612.h5
🗑️  已删除: epoch_017_loss_0.9113_acc_0.6116.h5
📋 已将最佳模型复制到: data/models/jaad/Transformer_depth/12Nov2025-02h51m22s/epoch_008_loss_0.5895_acc_0.6033.h5
🗑️  已删除: epoch_008_loss_0.5895_acc_0.6033.h5
🗑️  已删除: epoch_002_loss_3.3176_acc_0.6818.h5
🗑️  已删除: epoch_019_loss_1.4124_acc_0.6446.h5
🗑️  已删除: epoch_023_loss_0.9622_acc_0.6281.h5
🗑️  已删除: epoch_024_loss_0.7888_acc_0.5785.h5
🗑️  已删除: epoch_025_loss_1.4523_acc_0.5992.h5
🗑️  已删除: epoch_013_loss_0.6063_acc_0.5579.h5
🗑️  已删除空的epochs目录
🔄 模型目录已重命名:
   原目录: 12Nov2025-02h51m22s
   新目录: 12Nov2025-02h51m22s_acc_0.6693

================================================================================
🎯 测试结果汇总
================================================================================
总模型数量: 31
成功测试: 31
失败测试: 0

📊 性能统计:
平均准确率: 0.6336 (±0.0206)
平均AUC: 0.6667 (±0.0315)
平均F1: 0.7161 (±0.0229)

🏆 最佳模型:
最高准确率: epoch_008_loss_0.5895_acc_0.6033.h5 (Acc: 0.6693)
最高AUC: epoch_002_loss_3.3176_acc_0.6818.h5 (AUC: 0.7262)
最高F1: epoch_016_loss_1.2309_acc_0.6777.h5 (F1: 0.7546)

✅ 测试完成 (耗时: 11.0 分钟)

================================================================================
🎉 训练和测试管道完成!
================================================================================
模型目录: data/models/jaad/Transformer_depth/12Nov2025-02h51m22s
总耗时: 21.0 分钟
结束时间: 2025-11-12 03:12:15
================================================================================