(tf26) minshi@Legion:~/Pedestrian_Crossing_Intention_Prediction$  cd /home/minshi/Pedestrian_Crossing_Intention_Prediction ; /usr/bin/env /home/minshi/miniconda3/envs/tf26/bin/python /home/minshi/.vscode/extensions/ms-python.debugpy-2025.14.1-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 42763 -- /home/minshi/Pedestrian_Crossing_Intention_Prediction/train_and_test_all_epoch_pipeline.py -c config_files/my/my_jaad.yaml 
================================================================================
ðŸŽ¯ è®­ç»ƒå’Œæµ‹è¯•ç®¡é“å¯åŠ¨
================================================================================
é…ç½®æ–‡ä»¶: config_files/my/my_jaad.yaml
å¼€å§‹æ—¶é—´: 2025-11-12 04:08:50

ðŸš€ å¼€å§‹è®­ç»ƒ...
2025-11-12 04:08:54.010674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:08:54.014657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:08:54.014772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
config_files/my/my_jaad.yaml
model_opts {'model': 'Transformer_depth', 'obs_input_type': ['box', 'depth', 'vehicle_speed', 'ped_speed'], 'enlarge_ratio': 1.5, 'obs_length': 16, 'time_to_event': [30, 60], 'overlap': 0.8, 'balance_data': False, 'apply_class_weights': True, 'dataset': 'jaad', 'normalize_boxes': True, 'generator': True, 'fusion_point': 'early', 'fusion_method': 'sum'}
data_opts {'fstride': 1, 'sample_type': 'beh', 'subset': 'default', 'data_split_type': 'default', 'seq_type': 'crossing', 'min_track_size': 76}
net_opts {'num_hidden_units': 256, 'global_pooling': 'avg', 'regularizer_val': 0.0001, 'cell_type': 'gru', 'backbone': 'vgg16', 'dropout': 0.1}
train_opts {'batch_size': 2, 'epochs': 30, 'lr': 5e-05, 'learning_scheduler': {}}
---------------------------------------------------------
Generating action sequence data
fstride: 1
sample_type: beh
subset: default
height_rng: [0, inf]
squarify_ratio: 0
data_split_type: default
seq_type: crossing
min_track_size: 76
random_params: {'ratios': None, 'val_data': True, 'regen_data': False}
kfold_params: {'num_folds': 5, 'fold': 1}
---------------------------------------------------------
Generating database for jaad
jaad database loaded from /home/minshi/Pedestrian_Crossing_Intention_Prediction/JAAD/data_cache/jaad_database.pkl
---------------------------------------------------------
Generating crossing data
Split: train
Number of pedestrians: 324 
Total number of samples: 194 
---------------------------------------------------------
Generating action sequence data
fstride: 1
sample_type: beh
subset: default
height_rng: [0, inf]
squarify_ratio: 0
data_split_type: default
seq_type: crossing
min_track_size: 76
random_params: {'ratios': None, 'val_data': True, 'regen_data': False}
kfold_params: {'num_folds': 5, 'fold': 1}
---------------------------------------------------------
Generating database for jaad
jaad database loaded from /home/minshi/Pedestrian_Crossing_Intention_Prediction/JAAD/data_cache/jaad_database.pkl
---------------------------------------------------------
Generating crossing data
Split: val
Number of pedestrians: 48 
Total number of samples: 22 
---------------------------------------------------------
Generating action sequence data
fstride: 1
sample_type: beh
subset: default
height_rng: [0, inf]
squarify_ratio: 0
data_split_type: default
seq_type: crossing
min_track_size: 76
random_params: {'ratios': None, 'val_data': True, 'regen_data': False}
kfold_params: {'num_folds': 5, 'fold': 1}
---------------------------------------------------------
Generating database for jaad
jaad database loaded from /home/minshi/Pedestrian_Crossing_Intention_Prediction/JAAD/data_cache/jaad_database.pkl
---------------------------------------------------------
Generating crossing data
Split: test
Number of pedestrians: 276 
Total number of samples: 171 
[DataGenerator] auto class_weight -> {0: 0.8247422680412371, 1: 0.17525773195876287}
[DataGenerator] auto class_weight -> {0: 0.7272727272727273, 1: 0.2727272727272727}
2025-11-12 04:08:56.221951: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-12 04:08:56.223529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:08:56.223725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:08:56.223850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:08:56.569784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:08:56.569916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:08:56.569998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:08:56.570080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3567 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9

============================================================
ðŸ“Š MODEL PARAMETER STATISTICS
============================================================
Total parameters:        2,968,717
Trainable parameters:    2,968,711.0
Non-trainable parameters: 6.0
============================================================

/home/minshi/miniconda3/envs/tf26/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  warnings.warn(

ðŸš€ Training started!
ðŸ“ Models will be saved to: data/models/jaad/Transformer_depth/12Nov2025-04h08m56s
ðŸ“‹ å·²å¤åˆ¶ action_predict.py åˆ°æ¨¡åž‹ç›®å½•
2025-11-12 04:08:58.048226: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/30
2025-11-12 04:09:03.976814: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2025-11-12 04:09:04.348943: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100
1067/1067 [==============================] - 25s 18ms/step - loss: 9.7300 - cls_loss: 0.2145 - reg_loss: 0.0167 - intention_accuracy: 0.5127 - val_loss: 6.0898 - val_cls_loss: 0.2814 - val_reg_loss: 0.0032 - val_intention_accuracy: 0.2851
/home/minshi/miniconda3/envs/tf26/lib/python3.8/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
  warnings.warn('Custom mask layers require a config and must override '
[Sigma] Epoch 1: sigma_cls=0.6893 sigma_reg=0.6669
Epoch 2/30
1067/1067 [==============================] - 25s 23ms/step - loss: 4.1790 - cls_loss: 0.1836 - reg_loss: 0.0075 - intention_accuracy: 0.6312 - val_loss: 3.2231 - val_cls_loss: 0.2853 - val_reg_loss: 0.0047 - val_intention_accuracy: 0.6405
[Sigma] Epoch 2: sigma_cls=0.6812 sigma_reg=0.6410
Epoch 3/30
1067/1067 [==============================] - 25s 23ms/step - loss: 2.3528 - cls_loss: 0.1690 - reg_loss: 0.0050 - intention_accuracy: 0.6481 - val_loss: 2.0855 - val_cls_loss: 0.2713 - val_reg_loss: 0.0026 - val_intention_accuracy: 0.6488
[Sigma] Epoch 3: sigma_cls=0.6724 sigma_reg=0.6158
Epoch 4/30
1067/1067 [==============================] - 24s 23ms/step - loss: 1.4991 - cls_loss: 0.1614 - reg_loss: 0.0041 - intention_accuracy: 0.6621 - val_loss: 1.4367 - val_cls_loss: 0.2691 - val_reg_loss: 0.0026 - val_intention_accuracy: 0.6033
[Sigma] Epoch 4: sigma_cls=0.6633 sigma_reg=0.5914
Epoch 5/30
1067/1067 [==============================] - 24s 23ms/step - loss: 0.9220 - cls_loss: 0.1512 - reg_loss: 0.0034 - intention_accuracy: 0.6860 - val_loss: 1.1123 - val_cls_loss: 0.3295 - val_reg_loss: 0.0032 - val_intention_accuracy: 0.6157
[Sigma] Epoch 5: sigma_cls=0.6538 sigma_reg=0.5677
Epoch 6/30
 284/1067 [======>.......................] - ETA: 17s - loss: 0.6199 - cls_loss: 0.1416 - reg_loss: 0.0034 - intention_accurac                   286/1067 [=======>......................] - ETA: 17s - loss: 0.6176 - cls_loss: 0.1408 - reg_loss: 0.0033 - intention_accurac                                  288/1067 [=======>......................] - ETA: 17s - loss: 0.6176 - cls_loss: 0.1409 - reg_loss: 0.0034 - intention_accurac                                  291/1067 [=======>......................] - ETA: 17s - loss: 0.6152 - cls_loss: 0.1401 - reg_loss: 0.0034 - intention_accurac                                  294/1067 [=======>......................] - ETA: 17s - loss: 0.6127 - cls_loss: 0.1393 - reg_loss: 0.0034 - intention_accurac                                  297/1067 [=======>......................] - ETA: 17s - loss: 0.6121 - cls_loss: 0.1393 - reg_loss: 0.0034 - intention_accurac                                  299/1067 [=======>......................] - ETA: 17s - loss: 0.6108 - cls_loss: 0.1389 - reg_loss: 0.0034 - intention_accurac                                  302/1067 [=======>......................] - ETA: 16s - loss: 0.6084 - cls_loss: 0.1381 - reg_loss: 0.0034 - intention_accurac    305/1067 [=======>......................] - ETA: 16s - loss: 0.6060 - cls_loss: 0.1374 - reg_loss: 0.0033 - intention_accurac                      307/1067 [=======>......................] - ETA: 16s - loss: 0.6047 - cls_loss: 0.1370 - reg_loss: 0.0033 - intention_accurac                                  310/1067 [=======>......................] - ETA: 16s - loss: 0.6026 - cls_loss: 0.1363 - reg_loss: 0.0033 - intention_accurac                                  312/1067 [=======>......................] - ETA: 16s - loss: 0.6015 - cls_loss: 0.1360 - reg_loss: 0.0033 - intention_accurac                                  315/1067 [=======>......................] - ETA: 16s - loss: 0.6025 - cls_loss: 0.1367 - reg_loss: 0.0033 - intention_accurac                                  318/1067 [=======>......................] - ETA: 16s - loss: 0.6009 - cls_loss: 0.1362 - reg_loss: 0.0033 - intention_accurac                                  320/1067 [=======>......................] - ETA: 16s - loss: 0.6001 - cls_loss: 0.1361 - reg_loss: 0.0033 - intention_accurac                                  323/1067 [========>.....................] - ETA: 16s - loss: 0.5977 - cls_loss: 0.1353 - reg_loss: 0.0033 - intention_accurac       325/1067 [========>.....................] - ETA: 16s - loss: 0.5955 - cls_loss: 0.1345 - reg_loss: 0.0033 - intention_accurac                         328/1067 [========>.....................] - ETA: 16s - loss: 0.5941 - cls_loss: 0.1342 - reg_loss: 0.0033 - intention_accurac                                  330/1067 [========>.....................] - ETA: 16s - loss: 0.5922 - cls_loss: 0.1335 - reg_loss: 0.0033 - intention_accurac                                  332/1067 [========>.....................] - ETA: 16s - loss: 0.5941 - cls_loss: 0.1345 - reg_loss: 0.0033 - intention_accurac                                  335/1067 [========>.....................] - ETA: 16s - loss: 0.5964 - cls_loss: 0.1357 - reg_loss: 0.0033 - intention_accurac                                  338/1067 [========>.....................] - ETA: 16s - loss: 0.5942 - cls_loss: 0.1350 - reg_loss: 0.0032 - intention_accurac                                  341/1067 [========>.....................] - ETA: 16s - loss: 0.5955 - cls_loss: 0.1358 - reg_loss: 0.0032 - intention_accurac                                  344/1067 [========>.....................] - ETA: 16s - loss: 0.5962 - cls_loss: 0.1363 - reg_loss: 0.0032 - intention_accurac          347/1067 [========>.....................] - ETA: 15s - loss: 0.5955 - cls_loss: 0.1363 - reg_loss: 0.0032 - intention_accurac                            350/1067 [========>.....................] - ETA: 15s - loss: 0.5932 - cls_loss: 0.1355 - reg_loss: 0.0032 - intention_accurac                                  352/1067 [========>.....................] - ETA: 15s - loss: 0.5922 - cls_loss: 0.1353 - reg_loss: 0.0032 - intention_accurac                                  354/1067 [========>.....................] - ETA: 15s - loss: 0.5927 - cls_loss: 0.1356 - reg_loss: 0.0032 - intention_accurac                                  356/1067 [=========>....................] - ETA: 15s - loss: 0.5938 - cls_loss: 0.1362 - reg_loss: 0.0032 - intention_accurac                                  358/1067 [=========>....................] - ETA: 15s - loss: 0.5925 - cls_loss: 0.1358 - reg_loss: 0.0032 - intention_accurac                                  360/1067 [=========>....................] - ETA: 15s - loss: 0.5907 - cls_loss: 0.1352 - reg_loss: 0.0032 - intention_accurac                                  362/1067 [=========>....................] - ETA: 15s - loss: 0.5899 - cls_loss: 0.1350 - reg_loss: 0.0032 - intention_accurac             364/1067 [=========>....................] - ETA: 15s - loss: 0.5899 - cls_loss: 0.1352 - reg_loss: 0.0032 - intention_accurac                               367/1067 [=========>....................] - ETA: 15s - loss: 0.5892 - cls_loss: 0.1351 - reg_loss: 0.0032 - intention_accurac                                  369/1067 [=========>....................] - ETA: 15s - loss: 0.5883 - cls_loss: 0.1349 - reg_loss: 0.0032 - intention_accurac                                  371/1067 [=========>....................] - ETA: 15s - loss: 0.5876 - cls_loss: 0.1347 - reg_loss: 0.0032 - intention_accurac                                  373/1067 [=========>....................] - ETA: 15s - loss: 0.5899 - cls_loss: 0.1358 - reg_loss: 0.0033 - intention_accurac                                  376/1067 [=========>....................] - ETA: 15s - loss: 0.5882 - cls_loss: 0.1354 - reg_loss: 0.0033 - intention_accurac                                  378/1067 [=========>....................] - ETA: 15s - loss: 0.5895 - cls_loss: 0.1361 - reg_loss: 0.0033 - intention_accurac                                  380/1067 [=========>....................] - ETA: 15s - loss: 0.5889 - cls_loss: 0.1359 - reg_loss: 0.0034 - intention_accurac                382/1067 [=========>....................] - ETA: 15s - loss: 0.5884 - cls_loss: 0.1359 - reg_loss: 0.0034 - intention_accurac                                  385/1067 [=========>....................] - ETA: 15s - loss: 0.5907 - cls_loss: 0.1371 - reg_loss: 0.0033 - intention_accurac                                  388/1067 [=========>....................] - ETA: 15s - loss: 0.5898 - cls_loss: 0.1369 - reg_loss: 0.0033 - intention_accurac                                  391/1067 [=========>....................] - ETA: 15s - loss: 0.5910 - cls_loss: 0.1377 - reg_loss: 0.0033 - intention_accurac                                  394/1067 [==========>...................] - ETA: 15s - loss: 0.5905 - cls_loss: 0.1377 - reg_loss: 0.0033 - intention_accurac                                  397/1067 [==========>...................] - ETA: 15s - loss: 0.5894 - cls_loss: 0.1374 - reg_loss: 0.0033 - intention_accurac                                  400/1067 [==========>...................] - ETA: 15s - loss: 0.5872 - cls_loss: 0.1367 - reg_loss: 0.0033 - intention_accurac 403/1067 [==========>...................] - ETA: 14s - loss: 0.5877 - cls_loss: 0.1372 - reg_loss: 0.0033 - intention_accurac                   405/1067 [==========>...................] - ETA: 14s - loss: 0.5874 - cls_loss: 0.1372 - reg_loss: 0.0033 - intention_accurac                                  408/1067 [==========>...................] - ETA: 14s - loss: 0.5871 - cls_loss: 0.1373 - reg_loss: 0.0033 - intention_accurac                                  411/1067 [==========>...................] - ETA: 14s - loss: 0.5858 - cls_loss: 0.1370 - reg_loss: 0.0033 - intention_accurac                                  414/1067 [==========>...................] - ETA: 14s - loss: 0.5854 - cls_loss: 0.1370 - reg_loss: 0.0033 - intention_accurac                                  417/1067 [==========>...................] - ETA: 14s - loss: 0.5835 - cls_loss: 0.1365 - reg_loss: 0.0033 - intention_accurac                                  420/1067 [==========>...................] - ETA: 14s - loss: 0.5822 - cls_loss: 0.1362 - reg_loss: 0.0033 - intention_accurac                                  423/1067 [==========>...................] - ETA: 14s - loss: 0.5818 - cls_loss: 0.1363 - reg_loss: 0.0033 - intention_accurac    426/1067 [==========>...................] - ETA: 14s - loss: 0.5798 - cls_loss: 0.1356 - reg_loss: 0.0033 - intention_accurac                      428/1067 [===========>..................] - ETA: 14s - loss: 0.5855 - cls_loss: 0.1382 - reg_loss: 0.0033 - intention_accurac                                  431/1067 [===========>..................] - ETA: 14s - loss: 0.5858 - cls_loss: 0.1385 - reg_loss: 0.0033 - intention_accurac                                  433/1067 [===========>..................] - ETA: 14s - loss: 0.5853 - cls_loss: 0.1385 - reg_loss: 0.0033 - intention_accurac                                 1067/1067 [==============================] - 25s 23ms/step - loss: 0.4895 - cls_loss: 0.1437 - reg_loss: 0.0031 - intention_accuracy: 0.7104 - val_loss: 0.6882 - val_cls_loss: 0.2964 - val_reg_loss: 0.0044 - val_intention_accuracy: 0.6322
[Sigma] Epoch 6: sigma_cls=0.6440 sigma_reg=0.5447
Epoch 7/30
1067/1067 [==============================] - 25s 23ms/step - loss: 0.1736 - cls_loss: 0.1431 - reg_loss: 0.0027 - intention_accuracy: 0.7071 - val_loss: 0.5010 - val_cls_loss: 0.3276 - val_reg_loss: 0.0031 - val_intention_accuracy: 0.5950
[Sigma] Epoch 7: sigma_cls=0.6352 sigma_reg=0.5224
Epoch 8/30
1067/1067 [==============================] - 26s 24ms/step - loss: -0.0932 - cls_loss: 0.1323 - reg_loss: 0.0025 - intention_accuracy: 0.7291 - val_loss: 0.3987 - val_cls_loss: 0.3642 - val_reg_loss: 0.0024 - val_intention_accuracy: 0.6240
[Sigma] Epoch 8: sigma_cls=0.6255 sigma_reg=0.5008
Epoch 9/30
1067/1067 [==============================] - 24s 23ms/step - loss: -0.3020 - cls_loss: 0.1239 - reg_loss: 0.0022 - intention_accuracy: 0.7751 - val_loss: 0.2986 - val_cls_loss: 0.3810 - val_reg_loss: 0.0029 - val_intention_accuracy: 0.6364
[Sigma] Epoch 9: sigma_cls=0.6155 sigma_reg=0.4800
Epoch 10/30
1067/1067 [==============================] - 24s 23ms/step - loss: -0.4721 - cls_loss: 0.1159 - reg_loss: 0.0020 - intention_accuracy: 0.7887 - val_loss: 0.0466 - val_cls_loss: 0.3297 - val_reg_loss: 0.0021 - val_intention_accuracy: 0.6240
[Sigma] Epoch 10: sigma_cls=0.6051 sigma_reg=0.4598
Epoch 11/30
1067/1067 [==============================] - 25s 24ms/step - loss: -0.6197 - cls_loss: 0.1075 - reg_loss: 0.0018 - intention_accuracy: 0.8004 - val_loss: 0.4855 - val_cls_loss: 0.5166 - val_reg_loss: 0.0020 - val_intention_accuracy: 0.6033
[Sigma] Epoch 11: sigma_cls=0.5940 sigma_reg=0.4403
Epoch 12/30
1067/1067 [==============================] - 24s 22ms/step - loss: -0.7373 - cls_loss: 0.1034 - reg_loss: 0.0017 - intention_accuracy: 0.8224 - val_loss: 0.2550 - val_cls_loss: 0.4577 - val_reg_loss: 0.0022 - val_intention_accuracy: 0.5992
[Sigma] Epoch 12: sigma_cls=0.5843 sigma_reg=0.4215
Epoch 13/30
1067/1067 [==============================] - 25s 23ms/step - loss: -0.8636 - cls_loss: 0.0927 - reg_loss: 0.0016 - intention_accuracy: 0.8341 - val_loss: 0.6089 - val_cls_loss: 0.5924 - val_reg_loss: 0.0011 - val_intention_accuracy: 0.6033
[Sigma] Epoch 13: sigma_cls=0.5734 sigma_reg=0.4033
Epoch 14/30
1067/1067 [==============================] - 25s 24ms/step - loss: -0.9711 - cls_loss: 0.0866 - reg_loss: 0.0016 - intention_accuracy: 0.8397 - val_loss: 0.8680 - val_cls_loss: 0.6803 - val_reg_loss: 0.0016 - val_intention_accuracy: 0.6570
[Sigma] Epoch 14: sigma_cls=0.5619 sigma_reg=0.3858
Epoch 15/30
1067/1067 [==============================] - 26s 25ms/step - loss: -1.0481 - cls_loss: 0.0874 - reg_loss: 0.0015 - intention_accuracy: 0.8463 - val_loss: 0.8981 - val_cls_loss: 0.6910 - val_reg_loss: 0.0017 - val_intention_accuracy: 0.5661
[Sigma] Epoch 15: sigma_cls=0.5515 sigma_reg=0.3689
Epoch 16/30
1067/1067 [==============================] - 24s 23ms/step - loss: -1.1324 - cls_loss: 0.0846 - reg_loss: 0.0015 - intention_accuracy: 0.8435 - val_loss: 0.5890 - val_cls_loss: 0.6010 - val_reg_loss: 0.0016 - val_intention_accuracy: 0.5620
[Sigma] Epoch 16: sigma_cls=0.5420 sigma_reg=0.3526
Epoch 17/30
1067/1067 [==============================] - 26s 25ms/step - loss: -1.2230 - cls_loss: 0.0791 - reg_loss: 0.0014 - intention_accuracy: 0.8458 - val_loss: 0.4401 - val_cls_loss: 0.5592 - val_reg_loss: 0.0012 - val_intention_accuracy: 0.6694
[Sigma] Epoch 17: sigma_cls=0.5316 sigma_reg=0.3370
Epoch 18/30
1067/1067 [==============================] - 25s 24ms/step - loss: -1.2768 - cls_loss: 0.0827 - reg_loss: 0.0013 - intention_accuracy: 0.8510 - val_loss: 0.7213 - val_cls_loss: 0.6382 - val_reg_loss: 0.0012 - val_intention_accuracy: 0.6322
[Sigma] Epoch 18: sigma_cls=0.5229 sigma_reg=0.3219
Epoch 19/30
1067/1067 [==============================] - 24s 22ms/step - loss: -1.3852 - cls_loss: 0.0715 - reg_loss: 0.0013 - intention_accuracy: 0.8721 - val_loss: 0.4060 - val_cls_loss: 0.5523 - val_reg_loss: 0.0014 - val_intention_accuracy: 0.6860
[Sigma] Epoch 19: sigma_cls=0.5134 sigma_reg=0.3074
Epoch 20/30
1067/1067 [==============================] - 24s 23ms/step - loss: -1.4398 - cls_loss: 0.0739 - reg_loss: 0.0014 - intention_accuracy: 0.8674 - val_loss: 1.4981 - val_cls_loss: 0.8309 - val_reg_loss: 9.3712e-04 - val_intention_accuracy: 0.6612
[Sigma] Epoch 20: sigma_cls=0.5048 sigma_reg=0.2935
Epoch 21/30
1067/1067 [==============================] - 24s 23ms/step - loss: -1.5044 - cls_loss: 0.0732 - reg_loss: 0.0013 - intention_accuracy: 0.8655 - val_loss: -0.2837 - val_cls_loss: 0.3815 - val_reg_loss: 0.0012 - val_intention_accuracy: 0.6612
[Sigma] Epoch 21: sigma_cls=0.4965 sigma_reg=0.2802
Epoch 22/30
1067/1067 [==============================] - 24s 23ms/step - loss: -1.5847 - cls_loss: 0.0681 - reg_loss: 0.0013 - intention_accuracy: 0.8772 - val_loss: 0.5319 - val_cls_loss: 0.5801 - val_reg_loss: 0.0013 - val_intention_accuracy: 0.5496
[Sigma] Epoch 22: sigma_cls=0.4884 sigma_reg=0.2673
Epoch 23/30
1067/1067 [==============================] - 24s 22ms/step - loss: -1.6502 - cls_loss: 0.0670 - reg_loss: 0.0013 - intention_accuracy: 0.8744 - val_loss: 0.8212 - val_cls_loss: 0.6445 - val_reg_loss: 0.0013 - val_intention_accuracy: 0.6570
[Sigma] Epoch 23: sigma_cls=0.4804 sigma_reg=0.2550
Epoch 24/30
 614/1067 [================>.............] - ETA: 10s - loss: -1.7158 - cls_loss: 0.0631 - reg_loss: 0.0012 - intention_accura                                                         617/1067 [================>.............] - ETA: 10s - loss: -1.7167 - cls_loss: 0.0629 - reg_loss: 0.0012 - intention_accura                                                                        619/1067 [================>.............] - ETA: 10s - loss: -1.7165 - cls_loss: 0.0630 - reg_loss: 0.0012 - intention_accura                            622/1067 [================>.............] - ETA: 9s - loss: -1.7174 - cls_loss: 0.0628 - reg_loss: 0.0012 - intention_accurac                                                                        624/1067 [================>.............] - ETA: 9s - loss: -1.7183 - cls_loss: 0.0626 - reg_loss: 0.0012 - intention_accurac                                                                        626/1067 [================>.............] - ETA: 9s - loss: -1.7190 - cls_loss: 0.0625 - reg_loss: 0.0012 - intention_accurac                                                       628/1067 [================>.............] - ETA: 9s - loss: -1.7190 - cls_loss: 0.0625 - reg_loss: 0.0012 - intention_accurac                                                                        631/1067 [================>.............] - ETA: 9s - loss: -1.7170 - cls_loss: 0.0630 - reg_loss: 0.0012 - intention_accurac                          634/1067 [================>.............] - ETA: 9s - loss: -1.7181 - cls_loss: 0.0628 - reg_loss: 0.0012 - intention_accurac                                                                        637/1067 [================>.............] - ETA: 9s - loss: -1.7194 - cls_loss: 0.0625 - reg_loss: 0.0012 - intention_accurac                                                                        640/1067 [================>.............] - ETA: 9s - loss: -1.7208 - cls_loss: 0.0622 - reg_loss: 0.0012 - intention_accurac                                                     643/1067 [=================>............] - ETA: 9s - loss: -1.7212 - cls_loss: 0.0621 - reg_loss: 0.0012 - intention_accurac                                                                        646/1067 [=================>............] - ETA: 9s - loss: -1.7204 - cls_loss: 0.0623 - reg_loss: 0.0012 - intention_accurac                        649/1067 [=================>............] - ETA: 9s - loss: -1.7207 - cls_loss: 0.0623 - reg_loss: 0.0012 - intention_accurac                                                                        652/1067 [=================>............] - ETA: 9s - loss: -1.7209 - cls_loss: 0.0623 - reg_loss: 0.0012 - intention_accurac                                                                        655/1067 [=================>............] - ETA: 9s - loss: -1.7209 - cls_loss: 0.0623 - reg_loss: 0.0012 - intention_accurac                                                   658/1067 [=================>............] - ETA: 9s - loss: -1.7221 - cls_loss: 0.0620 - reg_loss: 0.0012 - intention_accurac                                                                        661/1067 [=================>............] - ETA: 9s - loss: -1.7190 - cls_loss: 0.0627 - reg_loss: 0.0012 - intention_accurac                      664/1067 [=================>............] - ETA: 9s - loss: -1.7204 - cls_loss: 0.0624 - reg_loss: 0.0012 - intention_accurac                                                                        667/1067 [=================>............] - ETA: 8s - loss: -1.7055 - cls_loss: 0.0658 - reg_loss: 0.0012 - intention_accurac                                                                        670/1067 [=================>............] - ETA: 8s - loss: -1.7063 - cls_loss: 0.0656 - reg_loss: 0.0012 - intention_accurac                                                 672/1067 [=================>............] - ETA: 8s - loss: -1.7068 - cls_loss: 0.0655 - reg_loss: 0.0012 - intention_accurac                                                                        675/1067 [=================>............] - ETA: 8s - loss: -1.7082 - cls_loss: 0.0652 - reg_loss: 0.0012 - intention_accurac                    678/1067 [==================>...........] - ETA: 8s - loss: -1.7083 - cls_loss: 0.0652 - reg_loss: 0.0012 - intention_accurac                                                                        681/1067 [==================>...........] - ETA: 8s - loss: -1.7069 - cls_loss: 0.0655 - reg_loss: 0.0012 - intention_accurac                                                                        684/1067 [==================>...........] - ETA: 8s - loss: -1.7082 - cls_loss: 0.0653 - reg_loss: 0.0012 - intention_accurac                                               687/1067 [==================>...........] - ETA: 8s - loss: -1.7072 - cls_loss: 0.0655 - reg_loss: 0.0012 - intention_accurac                                                                        690/1067 [==================>...........] - ETA: 8s - loss: -1.7070 - cls_loss: 0.0656 - reg_loss: 0.0012 - intention_accurac                  693/1067 [==================>...........] - ETA: 8s - loss: -1.7064 - cls_loss: 0.0657 - reg_loss: 0.0012 - intention_accurac                                                                        696/1067 [==================>...........] - ETA: 8s - loss: -1.7045 - cls_loss: 0.0662 - reg_loss: 0.0012 - intention_accurac                                                                        699/1067 [==================>...........] - ETA: 8s - loss: -1.7045 - cls_loss: 0.0662 - reg_loss: 0.0012 - intention_accurac                                             702/1067 [==================>...........] - ETA: 8s - loss: -1.7054 - cls_loss: 0.0660 - reg_loss: 0.0012 - intention_accurac                                                                        705/1067 [==================>...........] - ETA: 8s - loss: -1.7050 - cls_loss: 0.0661 - reg_loss: 0.0012 - intention_accurac                708/1067 [==================>...........] - ETA: 7s - loss: -1.7048 - cls_loss: 0.0662 - reg_loss: 0.0012 - intention_accurac                                                                        711/1067 [==================>...........] - ETA: 7s - loss: -1.7061 - cls_loss: 0.0659 - reg_loss: 0.0012 - intention_accurac                                                                        713/1067 [===================>..........] - ETA: 7s - loss: -1.7057 - cls_loss: 0.0660 - reg_loss: 0.0012 - intention_accurac                                           716/1067 [===================>..........] - ETA: 7s - loss: -1.7060 - cls_loss: 0.0660 - reg_loss: 0.0012 - intention_accurac                                                                        718/1067 [===================>..........] - ETA: 7s - loss: -1.7060 - cls_loss: 0.0660 - reg_loss: 0.0012 - intention_accurac              720/1067 [===================>..........] - ETA: 7s - loss: -1.7046 - cls_loss: 0.0663 - reg_loss: 0.0012 - intention_accurac                                                                      723/1067 [===================>..........] - ETA: 7s - loss: -1.7043 - cls_loss: 0.0664 - reg_loss: 0.0012 - intention_accurac                                                                        726/1067 [===================>..........] - ETA: 7s - loss: -1.7044 - cls_loss: 0.0664 - reg_loss: 0.0012 - intention_accurac                                         729/1067 [===================>..........] - ETA: 7s - loss: -1.7034 - cls_loss: 0.0666 - reg_loss: 0.0012 - intention_accurac                                                                        732/1067 [===================>..........] - ETA: 7s - loss: -1.7039 - cls_loss: 0.0665 - reg_loss: 0.0012 - intention_accurac            735/1067 [===================>..........] - ETA: 7s - loss: -1.7052 - cls_loss: 0.0663 - reg_loss: 0.0012 - intention_accurac                                                                    738/1067 [===================>..........] - ETA: 7s - loss: -1.7062 - cls_loss: 0.0661 - reg_loss: 0.0012 - intention_accurac                                                                        741/1067 [===================>..........] - ETA: 7s - loss: -1.7065 - cls_loss: 0.0660 - reg_loss: 0.0012 - intention_accurac                                       743/1067 [===================>..........] - ETA: 7s - loss: -1.7072 - cls_loss: 0.0658 - reg_loss: 0.0012 - intention_accurac                                                                        746/1067 [===================>..........] - ETA: 7s - loss: -1.7078 - cls_loss: 0.0657 - reg_loss: 0.0012 - intention_accurac          748/1067 [====================>.........] - ETA: 7s - loss: -1.7083 - cls_loss: 0.0656 - reg_loss: 0.0012 - intention_accurac                                                                  751/1067 [====================>.........] - ETA: 7s - loss: -1.7083 - cls_loss: 0.0657 - reg_loss: 0.0012 - intention_accurac                                                                        754/1067 [====================>.........] - ETA: 6s - loss: -1.7060 - cls_loss: 0.0662 - reg_loss: 0.0012 - intention_accurac                                     757/1067 [====================>.........] - ETA: 6s - loss: -1.7034 - cls_loss: 0.0668 - reg_loss: 0.0012 - intention_accurac                                                                        760/1067 [====================>.........] - ETA: 6s - loss: -1.7026 - cls_loss: 0.0670 - reg_loss: 0.0012 - intention_accurac        763/1067 [====================>.........] - ETA: 6s - loss: -1.7029 - cls_loss: 0.0669 - reg_loss: 0.0012 - intention_accurac                                                                765/1067 [====================>.........] - ETA: 6s - loss: -1.7032 - cls_loss: 0.0669 - reg_loss: 0.0012 - intention_accurac                                                                        767/1067 [====================>.........] - ETA: 6s - loss: -1.7037 - cls_loss: 0.0668 - reg_loss: 0.0012 - intention_accurac                                   769/1067 [====================>.........] - ETA: 6s - loss: -1.7044 - cls_loss: 0.0666 - reg_loss: 0.0012 - intention_accurac                                                                        772/1067 [====================>.........] - ETA: 6s - loss: -1.7048 - cls_loss: 0.0665 - reg_loss: 0.0012 - intention_accurac      775/1067 [====================>.........] - ETA: 6s - loss: -1.7038 - cls_loss: 0.0668 - reg_loss: 0.0012 - intention_accurac                                                              778/1067 [====================>.........] - ETA: 6s - loss: -1.7024 - cls_loss: 0.0671 - reg_loss: 0.0012 - intention_accurac                                                                        781/1067 [====================>.........] - ETA: 6s - loss: -1.7032 - cls_loss: 0.0670 - reg_loss: 0.0012 - intention_accurac                                 784/1067 [=====================>........] - ETA: 6s - loss: -1.7029 - cls_loss: 0.0671 - reg_loss: 0.0012 - intention_accurac                                                                        787/1067 [=====================>........] - ETA: 6s - loss: -1.7020 - cls_loss: 0.0673 - reg_loss: 0.0012 - intention_accurac   1067/1067 [==============================] - 25s 24ms/step - loss: -1.7128 - cls_loss: 0.0664 - reg_loss: 0.0013 - intention_accuracy: 0.8777 - val_loss: 1.1053 - val_cls_loss: 0.7028 - val_reg_loss: 0.0010 - val_intention_accuracy: 0.6446
[Sigma] Epoch 24: sigma_cls=0.4728 sigma_reg=0.2432
Epoch 25/30
1067/1067 [==============================] - 24s 23ms/step - loss: -1.7850 - cls_loss: 0.0632 - reg_loss: 0.0013 - intention_accuracy: 0.8871 - val_loss: 0.6709 - val_cls_loss: 0.6010 - val_reg_loss: 0.0012 - val_intention_accuracy: 0.6074
[Sigma] Epoch 25: sigma_cls=0.4650 sigma_reg=0.2319
Epoch 26/30
1067/1067 [==============================] - 25s 23ms/step - loss: -1.8424 - cls_loss: 0.0632 - reg_loss: 0.0012 - intention_accuracy: 0.8885 - val_loss: 0.2757 - val_cls_loss: 0.5146 - val_reg_loss: 0.0014 - val_intention_accuracy: 0.6488
[Sigma] Epoch 26: sigma_cls=0.4588 sigma_reg=0.2211
Epoch 27/30
1067/1067 [==============================] - 25s 24ms/step - loss: -1.9156 - cls_loss: 0.0597 - reg_loss: 0.0013 - intention_accuracy: 0.8964 - val_loss: 1.1754 - val_cls_loss: 0.6977 - val_reg_loss: 0.0012 - val_intention_accuracy: 0.6653
[Sigma] Epoch 27: sigma_cls=0.4522 sigma_reg=0.2107
Epoch 28/30
1067/1067 [==============================] - 25s 23ms/step - loss: -1.9794 - cls_loss: 0.0586 - reg_loss: 0.0012 - intention_accuracy: 0.8983 - val_loss: 1.3919 - val_cls_loss: 0.7341 - val_reg_loss: 8.5633e-04 - val_intention_accuracy: 0.6736
[Sigma] Epoch 28: sigma_cls=0.4454 sigma_reg=0.2007
Epoch 29/30
 627/1067 [================>.............] - ETA: 9s - loss: -2.0323 - cls_loss: 0.0573 - reg_loss: 0.0012 - intention_accurac                        629/1067 [================>.............] - ETA: 9s - loss: -2.0332 - cls_loss: 0.0571 - reg_loss: 0.0012 - intention_accurac                                       631/1067 [================>.............] - ETA: 9s - loss: -2.0342 - cls_loss: 0.0570 - reg_loss: 0.0012 - intention_accurac                                       633/1067 [================>.............] - ETA: 9s - loss: -2.0318 - cls_loss: 0.0574 - reg_loss: 0.0012 - intention_accurac                                       635/1067 [================>.............] - ETA: 9s - loss: -2.0314 - cls_loss: 0.0575 - reg_loss: 0.0012 - intention_accurac                                       638/1067 [================>.............] - ETA: 9s - loss: -2.0316 - cls_loss: 0.0575 - reg_loss: 0.0012 - intention_accurac                                       641/1067 [=================>............] - ETA: 9s - loss: -2.0303 - cls_loss: 0.0578 - reg_loss: 0.0012 - intention_accurac                     644/1067 [=================>............] - ETA: 8s - loss: -2.0313 - cls_loss: 0.0576 - reg_loss: 0.0011 - intention_accurac                                       646/1067 [=================>............] - ETA: 8s - loss: -2.0322 - cls_loss: 0.0574 - reg_loss: 0.0011 - intention_accurac                                       649/1067 [=================>............] - ETA: 8s - loss: -2.0331 - cls_loss: 0.0573 - reg_loss: 0.0011 - intention_accurac                                       652/1067 [=================>............] - ETA: 8s - loss: -2.0336 - cls_loss: 0.0572 - reg_loss: 0.0011 - intention_accurac                                       655/1067 [=================>............] - ETA: 8s - loss: -2.0293 - cls_loss: 0.0580 - reg_loss: 0.0011 - intention_accurac                                       658/1067 [=================>............] - ETA: 8s - loss: -2.0304 - cls_loss: 0.0579 - reg_loss: 0.0011 - intention_accurac                  661/1067 [=================>............] - ETA: 8s - loss: -2.0295 - cls_loss: 0.0581 - reg_loss: 0.0011 - intention_accurac                                       664/1067 [=================>............] - ETA: 8s - loss: -2.0273 - cls_loss: 0.0584 - reg_loss: 0.0012 - intention_accurac                                       667/1067 [=================>............] - ETA: 8s - loss: -2.0266 - cls_loss: 0.0586 - reg_loss: 0.0012 - intention_accurac                                       670/1067 [=================>............] - ETA: 8s - loss: -2.0262 - cls_loss: 0.0587 - reg_loss: 0.0012 - intention_accurac                                       673/1067 [=================>............] - ETA: 8s - loss: -2.0246 - cls_loss: 0.0590 - reg_loss: 0.0012 - intention_accurac                                       675/1067 [=================>............] - ETA: 8s - loss: -2.0255 - cls_loss: 0.0588 - reg_loss: 0.0012 - intention_accurac               677/1067 [==================>...........] - ETA: 8s - loss: -2.0215 - cls_loss: 0.0596 - reg_loss: 0.0012 - intention_accurac                                      680/1067 [==================>...........] - ETA: 8s - loss: -2.0220 - cls_loss: 0.0595 - reg_loss: 0.0012 - intention_accurac                                       682/1067 [==================>...........] - ETA: 8s - loss: -2.0205 - cls_loss: 0.0598 - reg_loss: 0.0012 - intention_accurac                                       685/1067 [==================>...........] - ETA: 8s - loss: -2.0205 - cls_loss: 0.0599 - reg_loss: 0.0012 - intention_accurac                                       688/1067 [==================>...........] - ETA: 8s - loss: -2.0217 - cls_loss: 0.0596 - reg_loss: 0.0012 - intention_accurac                                       691/1067 [==================>...........] - ETA: 8s - loss: -2.0197 - cls_loss: 0.0600 - reg_loss: 0.0012 - intention_accurac            694/1067 [==================>...........] - ETA: 7s - loss: -2.0210 - cls_loss: 0.0598 - reg_loss: 0.0012 - intention_accurac                                   697/1067 [==================>...........] - ETA: 7s - loss: -2.0193 - cls_loss: 0.0601 - reg_loss: 0.0012 - intention_accurac                                       700/1067 [==================>...........] - ETA: 7s - loss: -2.0186 - cls_loss: 0.0602 - reg_loss: 0.0012 - intention_accurac                                       703/1067 [==================>...........] - ETA: 7s - loss: -2.0181 - cls_loss: 0.0603 - reg_loss: 0.0012 - intention_accurac                                       706/1067 [==================>...........] - ETA: 7s - loss: -2.0159 - cls_loss: 0.0608 - reg_loss: 0.0012 - intention_accurac                                       709/1067 [==================>...........] - ETA: 7s - loss: -2.0167 - cls_loss: 0.0606 - reg_loss: 0.0012 - intention_accurac         711/1067 [==================>...........] - ETA: 7s - loss: -2.0144 - cls_loss: 0.0611 - reg_loss: 0.0012 - intention_accurac                                713/1067 [===================>..........] - ETA: 7s - loss: -2.0129 - cls_loss: 0.0614 - reg_loss: 0.0012 - intention_accurac                                       715/1067 [===================>..........] - ETA: 7s - loss: -2.0130 - cls_loss: 0.0614 - reg_loss: 0.0012 - intention_accurac                                       718/1067 [===================>..........] - ETA: 7s - loss: -2.0129 - cls_loss: 0.0614 - reg_loss: 0.0012 - intention_accurac                                       721/1067 [===================>..........] - ETA: 7s - loss: -2.0117 - cls_loss: 0.0617 - reg_loss: 0.0012 - intention_accurac                                       723/1067 [===================>..........] - ETA: 7s - loss: -2.0125 - cls_loss: 0.0615 - reg_loss: 0.0012 - intention_accurac      726/1067 [===================>..........] - ETA: 7s - loss: -2.0117 - cls_loss: 0.0617 - reg_loss: 0.0012 - intention_accurac                             729/1067 [===================>..........] - ETA: 7s - loss: -2.0120 - cls_loss: 0.0616 - reg_loss: 0.0012 - intention_accurac                                       732/1067 [===================>..........] - ETA: 7s - loss: -2.0130 - cls_loss: 0.0615 - reg_loss: 0.0012 - intention_accurac                                       735/1067 [===================>..........] - ETA: 7s - loss: -2.0135 - cls_loss: 0.0614 - reg_loss: 0.0012 - intention_accurac                                       737/1067 [===================>..........] - ETA: 7s - loss: -2.0126 - cls_loss: 0.0616 - reg_loss: 0.0012 - intention_accurac                                       740/1067 [===================>..........] - ETA: 7s - loss: -2.0126 - cls_loss: 0.0616 - reg_loss: 0.0012 - intention_accurac   743/1067 [===================>..........] - ETA: 6s - loss: -2.0130 - cls_loss: 0.0615 - reg_loss: 0.0012 - intention_accurac                          746/1067 [===================>..........] - ETA: 6s - loss: -2.0118 - cls_loss: 0.0618 - reg_loss: 0.0012 - intention_accurac                                       749/1067 [====================>.........] - ETA: 6s - loss: -2.0124 - cls_loss: 0.0617 - reg_loss: 0.0012 - intention_accurac                                       752/1067 [====================>.........] - ETA: 6s - loss: -2.0136 - cls_loss: 0.0615 - reg_loss: 0.0012 - intention_accurac                                       755/1067 [====================>.........] - ETA: 6s - loss: -2.0117 - cls_loss: 0.0619 - reg_loss: 0.0012 - intention_accurac                                       758/1067 [====================>.........] - ETA: 6s - loss: -2.0094 - cls_loss: 0.0623 - reg_loss: 0.0012 - intention_accurac                                       761/1067 [====================>.........] - ETA: 6s - loss: -2.0102 - cls_loss: 0.0622 - reg_loss: 0.0012 - intention_accurac                       764/1067 [====================>.........] - ETA: 6s - loss: -2.0103 - cls_loss: 0.0621 - reg_loss: 0.0012 - intention_accurac                                       768/1067 [====================>.........] - ETA: 6s - loss: -2.0118 - cls_loss: 0.0619 - reg_loss: 0.0012 - intention_accurac                                       771/1067 [====================>.........] - ETA: 6s - loss: -2.0112 - cls_loss: 0.0620 - reg_loss: 0.0012 - intention_accurac                                       773/1067 [====================>.........] - ETA: 6s - loss: -2.0101 - cls_loss: 0.0622 - reg_loss: 0.0012 - intention_accurac                                       776/1067 [====================>.........] - ETA: 6s - loss: -2.0105 - cls_loss: 0.0622 - reg_loss: 0.0012 - intention_accurac                                       778/1067 [====================>.........] - ETA: 6s - loss: -2.0107 - cls_loss: 0.0622 - reg_loss: 0.0012 - intention_accurac                    780/1067 [====================>.........] - ETA: 6s - loss: -2.0113 - cls_loss: 0.0621 - reg_loss: 0.0012 - intention_accurac                                       783/1067 [=====================>........] - ETA: 6s - loss: -2.0125 - cls_loss: 0.0618 - reg_loss: 0.0012 - intention_accurac                                       785/1067 [=====================>........] - ETA: 6s - loss: -2.0122 - cls_loss: 0.0619 - reg_loss: 0.0012 - intention_accurac                                       787/1067 [=====================>........] - ETA: 6s - loss: -2.0130 - cls_loss: 0.0618 - reg_loss: 0.0012 - intention_accurac                                       790/1067 [=====================>........] - ETA: 5s - loss: -2.0131 - cls_loss: 0.0618 - reg_loss: 0.0012 - intention_accurac                                       793/1067 [=====================>........] - ETA: 5s - loss: -2.0129 - cls_loss: 0.0618 - reg_loss: 0.0012 - intention_accurac                 796/1067 [=====================>........] - ETA: 5s - loss: -2.0132 - cls_loss: 0.0618 - reg_loss: 0.0012 - intention_accurac                                       798/1067 [=====================>........] - ETA: 5s - loss: -2.0122 - cls_loss: 0.0620 - reg_loss: 0.0012 - intention_accurac                                       800/1067 [=====================>........] - ETA: 5s - loss: -2.0131 - cls_loss: 0.0618 - reg_loss: 0.0012 - intention_accurac                                       802/1067 [=====================>........] - ETA: 5s - loss: -2.0126 - cls_loss: 0.0619 - reg_loss: 0.0012 - intention_accurac                                       804/1067 [=====================>........] - ETA: 5s - loss: -2.0134 - cls_loss: 0.0618 - reg_loss: 0.0012 - intention_accurac                                       806/1067 [=====================>........] - ETA: 5s - loss: -2.0135 - cls_loss: 0.0618 - reg_loss: 0.0012 - intention_accurac              809/1067 [=====================>........] - ETA: 5s - loss: -2.0135 - cls_loss: 0.0618 - reg_loss: 0.0012 - intention_accurac                                     812/1067 [=====================>........] - ETA: 5s - loss: -2.0145 - cls_loss: 0.0616 - reg_loss: 0.0012 - intention_accurac                                       815/1067 [=====================>........] - ETA: 5s - loss: -2.0151 - cls_loss: 0.0615 - reg_loss: 0.0012 - intention_accurac                                       818/1067 [=====================>........] - ETA: 5s - loss: -2.0155 - cls_loss: 0.0614 - reg_loss: 0.0012 - intention_accurac                                       821/1067 [======================>.......] - ETA: 5s - loss: -2.0165 - cls_loss: 0.0613 - reg_loss: 0.0012 - intention_accurac                                       824/1067 [======================>.......] - ETA: 5s - loss: -2.0177 - cls_loss: 0.0611 - reg_loss: 0.0012 - intention_accurac           827/1067 [======================>.......] - ETA: 5s - loss: -2.0166 - cls_loss: 0.0613 - reg_loss: 0.0012 - intention_accurac                                  830/1067 [======================>.......] - ETA: 5s - loss: -2.0164 - cls_loss: 0.0613 - reg_loss: 0.0012 - intention_accurac                                       833/1067 [======================>.......] - ETA: 5s - loss: -2.0170 - cls_loss: 0.0612 - reg_loss: 0.0012 - intention_accurac                                       836/1067 [======================>.......] - ETA: 5s - loss: -2.0177 - cls_loss: 0.0611 - reg_loss: 0.0012 - intention_accurac                                       838/1067 [======================>.......] - ETA: 4s - loss: -2.0184 - cls_loss: 0.0610 - reg_loss: 0.0012 - intention_accurac                                       840/1067 [======================>.......] - ETA: 4s - loss: -2.0189 - cls_loss: 0.0609 - reg_loss: 0.0012 - intention_accurac        843/1067 [======================>.......] - ETA: 4s - loss: -2.0178 - cls_loss: 0.0611 - reg_loss: 0.0012 - intention_accurac                               845/1067 [======================>.......] - ETA: 4s - loss: -2.0185 - cls_loss: 0.0610 - reg_loss: 0.0012 - intention_accurac                                      1067/1067 [==============================] - 25s 23ms/step - loss: -2.0076 - cls_loss: 0.0641 - reg_loss: 0.0012 - intention_accuracy: 0.8796 - val_loss: 1.6372 - val_cls_loss: 0.7739 - val_reg_loss: 0.0011 - val_intention_accuracy: 0.4917
[Sigma] Epoch 29: sigma_cls=0.4397 sigma_reg=0.1912
Epoch 30/30
1067/1067 [==============================] - 24s 23ms/step - loss: -2.0783 - cls_loss: 0.0612 - reg_loss: 0.0011 - intention_accuracy: 0.8908 - val_loss: 1.1843 - val_cls_loss: 0.6811 - val_reg_loss: 9.6439e-04 - val_intention_accuracy: 0.6446
[Sigma] Epoch 30: sigma_cls=0.4340 sigma_reg=0.1821

ðŸŽ¯ Training completed!
ðŸ“ All epoch models saved in: data/models/jaad/Transformer_depth/12Nov2025-04h08m56s/epochs
Train model is saved to data/models/jaad/Transformer_depth/12Nov2025-04h08m56s/model.h5
Available metrics: ['loss', 'cls_loss', 'reg_loss', 'intention_accuracy', 'val_loss', 'val_cls_loss', 'val_reg_loss', 'val_intention_accuracy', 'sigma_cls', 'val_sigma_cls', 'sigma_reg', 'val_sigma_reg']
Training plots saved to model directory
Wrote configs to data/models/jaad/Transformer_depth/12Nov2025-04h08m56s/configs.yaml
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 22s 11ms/step

======================================================================
ðŸŽ¯ MODEL TEST RESULTS ðŸŽ¯
======================================================================
Accuracy:   0.5944
AUC:        0.5366
F1-Score:   0.7028
Precision:  0.6489
Recall:     0.7664
======================================================================

Model saved to data/models/jaad/Transformer_depth/12Nov2025-04h08m56s/

âœ… è®­ç»ƒå®Œæˆ (è€—æ—¶: 13.2 åˆ†é’Ÿ)
ðŸ” æŸ¥æ‰¾æœ€æ–°æ¨¡åž‹ç›®å½•...
ðŸ“ æ‰¾åˆ°æ¨¡åž‹ç›®å½•: data/models/jaad/Transformer_depth/12Nov2025-04h08m56s

ðŸ§ª å¼€å§‹æµ‹è¯•æ¨¡åž‹...
2025-11-12 04:22:06.076630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:22:06.083252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:22:06.083456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
ðŸš€ å¼€å§‹æµ‹è¯•æ¨¡åž‹ç›®å½•: data/models/jaad/Transformer_depth/12Nov2025-04h08m56s
âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ
âœ… æ•°æ®é›†åˆå§‹åŒ–æˆåŠŸ
ðŸ”„ ç”Ÿæˆæµ‹è¯•æ•°æ®...
---------------------------------------------------------
Generating action sequence data
fstride: 1
sample_type: beh
subset: default
height_rng: [0, inf]
squarify_ratio: 0
data_split_type: default
seq_type: crossing
min_track_size: 76
random_params: {'ratios': None, 'val_data': True, 'regen_data': False}
kfold_params: {'num_folds': 5, 'fold': 1}
---------------------------------------------------------
Generating database for jaad
jaad database loaded from /home/minshi/Pedestrian_Crossing_Intention_Prediction/JAAD/data_cache/jaad_database.pkl
---------------------------------------------------------
Generating crossing data
Split: test
Number of pedestrians: 276 
Total number of samples: 171 
âœ… æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ
ðŸ“ æ‰¾åˆ° 31 ä¸ªæ¨¡åž‹æ–‡ä»¶

============================================================
è¿›åº¦: 1/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_001_loss_6.0898_acc_0.2851.h5
2025-11-12 04:22:07.671279: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-12 04:22:07.673794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:22:07.674039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:22:07.674175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:22:08.216446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:22:08.216829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:22:08.216973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-11-12 04:22:08.217245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3881 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
2025-11-12 04:22:10.179822: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-11-12 04:22:12.194219: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2025-11-12 04:22:12.859817: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100
1881/1881 [==============================] - 23s 11ms/step
âœ… å‡†ç¡®çŽ‡: 0.4083, AUC: 0.6982, F1: 0.1031

============================================================
è¿›åº¦: 2/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_002_loss_3.2231_acc_0.6405.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6635, AUC: 0.7360, F1: 0.7024

============================================================
è¿›åº¦: 3/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_003_loss_2.0855_acc_0.6488.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6778, AUC: 0.7277, F1: 0.7356

============================================================
è¿›åº¦: 4/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_004_loss_1.4367_acc_0.6033.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6587, AUC: 0.7322, F1: 0.6887

============================================================
è¿›åº¦: 5/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_005_loss_1.1123_acc_0.6157.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6699, AUC: 0.7400, F1: 0.7069

============================================================
è¿›åº¦: 6/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_006_loss_0.6882_acc_0.6322.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 11ms/step
âœ… å‡†ç¡®çŽ‡: 0.6614, AUC: 0.7241, F1: 0.7000

============================================================
è¿›åº¦: 7/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_007_loss_0.5010_acc_0.5950.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6624, AUC: 0.7147, F1: 0.7000

============================================================
è¿›åº¦: 8/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_008_loss_0.3987_acc_0.6240.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6550, AUC: 0.7048, F1: 0.7096

============================================================
è¿›åº¦: 9/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_009_loss_0.2986_acc_0.6364.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 22s 11ms/step
âœ… å‡†ç¡®çŽ‡: 0.6502, AUC: 0.7028, F1: 0.7049

============================================================
è¿›åº¦: 10/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_010_loss_0.0466_acc_0.6240.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 22s 11ms/step
âœ… å‡†ç¡®çŽ‡: 0.6422, AUC: 0.6836, F1: 0.7057

============================================================
è¿›åº¦: 11/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_011_loss_0.4855_acc_0.6033.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6481, AUC: 0.6826, F1: 0.7144

============================================================
è¿›åº¦: 12/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_012_loss_0.2550_acc_0.5992.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6337, AUC: 0.6878, F1: 0.7059

============================================================
è¿›åº¦: 13/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_013_loss_0.6089_acc_0.6033.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 11ms/step
âœ… å‡†ç¡®çŽ‡: 0.6321, AUC: 0.6688, F1: 0.7068

============================================================
è¿›åº¦: 14/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_014_loss_0.8680_acc_0.6570.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 9ms/step
âœ… å‡†ç¡®çŽ‡: 0.5859, AUC: 0.6347, F1: 0.6998

============================================================
è¿›åº¦: 15/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_015_loss_0.8981_acc_0.5661.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6305, AUC: 0.6773, F1: 0.7134

============================================================
è¿›åº¦: 16/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_016_loss_0.5890_acc_0.5620.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6348, AUC: 0.6637, F1: 0.7174

============================================================
è¿›åº¦: 17/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_017_loss_0.4401_acc_0.6694.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.5837, AUC: 0.6364, F1: 0.6957

============================================================
è¿›åº¦: 18/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_018_loss_0.7213_acc_0.6322.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.5901, AUC: 0.6281, F1: 0.6944

============================================================
è¿›åº¦: 19/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_019_loss_0.4060_acc_0.6860.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6103, AUC: 0.6502, F1: 0.7142

============================================================
è¿›åº¦: 20/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_020_loss_1.4981_acc_0.6612.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 9ms/step
âœ… å‡†ç¡®çŽ‡: 0.6465, AUC: 0.6807, F1: 0.7449

============================================================
è¿›åº¦: 21/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_021_loss_-0.2837_acc_0.6612.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6077, AUC: 0.6570, F1: 0.6843

============================================================
è¿›åº¦: 22/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_022_loss_0.5319_acc_0.5496.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 9ms/step
âœ… å‡†ç¡®çŽ‡: 0.6342, AUC: 0.6525, F1: 0.6998

============================================================
è¿›åº¦: 23/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_023_loss_0.8212_acc_0.6570.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.5742, AUC: 0.6062, F1: 0.6951

============================================================
è¿›åº¦: 24/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_024_loss_1.1053_acc_0.6446.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 21s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6401, AUC: 0.6833, F1: 0.7336

============================================================
è¿›åº¦: 25/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_025_loss_0.6709_acc_0.6074.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 22s 11ms/step
âœ… å‡†ç¡®çŽ‡: 0.5944, AUC: 0.6357, F1: 0.6988

============================================================
è¿›åº¦: 26/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_026_loss_0.2757_acc_0.6488.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 24s 12ms/step
âœ… å‡†ç¡®çŽ‡: 0.6225, AUC: 0.6350, F1: 0.7146

============================================================
è¿›åº¦: 27/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_027_loss_1.1754_acc_0.6653.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.6082, AUC: 0.6348, F1: 0.7285

============================================================
è¿›åº¦: 28/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_028_loss_1.3919_acc_0.6736.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.5949, AUC: 0.6249, F1: 0.7176

============================================================
è¿›åº¦: 29/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_029_loss_1.6372_acc_0.4917.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 9ms/step
âœ… å‡†ç¡®çŽ‡: 0.5439, AUC: 0.5592, F1: 0.6299

============================================================
è¿›åº¦: 30/31

ðŸ” æµ‹è¯•æ¨¡åž‹: epoch_030_loss_1.1843_acc_0.6446.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 19s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.5944, AUC: 0.6255, F1: 0.7028

============================================================
è¿›åº¦: 31/31

ðŸ” æµ‹è¯•æ¨¡åž‹: model.h5
[DataGenerator] auto class_weight -> {0: 0.6257309941520468, 1: 0.3742690058479532}
1881/1881 [==============================] - 20s 10ms/step
âœ… å‡†ç¡®çŽ‡: 0.5944, AUC: 0.6255, F1: 0.7028

ðŸ“Š ç»“æžœå·²ä¿å­˜åˆ°: data/models/jaad/Transformer_depth/12Nov2025-04h08m56s/test_results_20251112_043341.csv
ðŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: data/models/jaad/Transformer_depth/12Nov2025-04h08m56s/test_report_20251112_043341.txt

ðŸ† å‡†ç¡®çŽ‡æœ€é«˜çš„æ¨¡åž‹: epoch_003_loss_2.0855_acc_0.6488.h5 (å‡†ç¡®çŽ‡: 0.6778)
ðŸ—‘ï¸  å¼€å§‹æ¸…ç†epochsç›®å½•ï¼Œå°†åˆ é™¤ 30 ä¸ªæ¨¡åž‹æ–‡ä»¶...
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_019_loss_0.4060_acc_0.6860.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_016_loss_0.5890_acc_0.5620.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_006_loss_0.6882_acc_0.6322.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_023_loss_0.8212_acc_0.6570.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_026_loss_0.2757_acc_0.6488.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_015_loss_0.8981_acc_0.5661.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_027_loss_1.1754_acc_0.6653.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_008_loss_0.3987_acc_0.6240.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_021_loss_-0.2837_acc_0.6612.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_009_loss_0.2986_acc_0.6364.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_025_loss_0.6709_acc_0.6074.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_030_loss_1.1843_acc_0.6446.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_028_loss_1.3919_acc_0.6736.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_022_loss_0.5319_acc_0.5496.h5
ðŸ“‹ å·²å°†æœ€ä½³æ¨¡åž‹å¤åˆ¶åˆ°: data/models/jaad/Transformer_depth/12Nov2025-04h08m56s/epoch_003_loss_2.0855_acc_0.6488.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_003_loss_2.0855_acc_0.6488.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_014_loss_0.8680_acc_0.6570.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_018_loss_0.7213_acc_0.6322.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_007_loss_0.5010_acc_0.5950.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_013_loss_0.6089_acc_0.6033.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_020_loss_1.4981_acc_0.6612.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_010_loss_0.0466_acc_0.6240.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_002_loss_3.2231_acc_0.6405.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_005_loss_1.1123_acc_0.6157.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_024_loss_1.1053_acc_0.6446.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_011_loss_0.4855_acc_0.6033.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_001_loss_6.0898_acc_0.2851.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_012_loss_0.2550_acc_0.5992.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_017_loss_0.4401_acc_0.6694.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_029_loss_1.6372_acc_0.4917.h5
ðŸ—‘ï¸  å·²åˆ é™¤: epoch_004_loss_1.4367_acc_0.6033.h5
ðŸ—‘ï¸  å·²åˆ é™¤ç©ºçš„epochsç›®å½•
ðŸ”„ æ¨¡åž‹ç›®å½•å·²é‡å‘½å:
   åŽŸç›®å½•: 12Nov2025-04h08m56s
   æ–°ç›®å½•: 12Nov2025-04h08m56s_acc_0.6778

================================================================================
ðŸŽ¯ æµ‹è¯•ç»“æžœæ±‡æ€»
================================================================================
æ€»æ¨¡åž‹æ•°é‡: 31
æˆåŠŸæµ‹è¯•: 31
å¤±è´¥æµ‹è¯•: 0

ðŸ“Š æ€§èƒ½ç»Ÿè®¡:
å¹³å‡å‡†ç¡®çŽ‡: 0.6179 (Â±0.0507)
å¹³å‡AUC: 0.6682 (Â±0.0434)
å¹³å‡F1: 0.6862 (Â±0.1099)

ðŸ† æœ€ä½³æ¨¡åž‹:
æœ€é«˜å‡†ç¡®çŽ‡: epoch_003_loss_2.0855_acc_0.6488.h5 (Acc: 0.6778)
æœ€é«˜AUC: epoch_005_loss_1.1123_acc_0.6157.h5 (AUC: 0.7400)
æœ€é«˜F1: epoch_020_loss_1.4981_acc_0.6612.h5 (F1: 0.7449)

âœ… æµ‹è¯•å®Œæˆ (è€—æ—¶: 11.7 åˆ†é’Ÿ)

================================================================================
ðŸŽ‰ è®­ç»ƒå’Œæµ‹è¯•ç®¡é“å®Œæˆ!
================================================================================
æ¨¡åž‹ç›®å½•: data/models/jaad/Transformer_depth/12Nov2025-04h08m56s
æ€»è€—æ—¶: 24.9 åˆ†é’Ÿ
ç»“æŸæ—¶é—´: 2025-11-12 04:33:43
================================================================================